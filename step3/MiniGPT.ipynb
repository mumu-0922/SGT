{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 3: Mini GPT â€” ä» Decoder-Only åˆ° LLaMA\n\n> **ç›®æ ‡**ï¼šç†è§£ NTP è®­ç»ƒ + LLM å¸¸è§æ”¹åŠ¨ï¼Œä¸ºçœ‹æ‡‚ MiniOneRec çš„ sft.py / rl.py æ‰“åŸºç¡€\n>\n> **å‰ç½®**ï¼šStep 2 å·²å­¦å®Œ Transformerã€RoPEã€Causal Mask\n>\n> **å‚è€ƒä¹¦ç›®**ï¼šå¤æ—¦å¤§å­¦ã€Šå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼šä»ç†è®ºåˆ°å®è·µã€‹ç¬¬äºŒç‰ˆ Â· ç¬¬ 2 ç« \n\n### ğŸ“– ä¹¦æœ¬ç« èŠ‚å¯¹ç…§è¡¨\n\n| ä¹¦æœ¬ç« èŠ‚ | é¡µç  | Notebook å¯¹åº” |\n|---|---|---|\n| 2.1.1 åµŒå…¥è¡¨ç¤ºå±‚ | p18 | Section 3: Token Embedding |\n| 2.1.2 æ³¨æ„åŠ›å±‚ | p19 | Section 3/7: MHA â†’ GQA |\n| 2.1.3 å‰é¦ˆå±‚ | p23 | Section 6: ReLU FFN â†’ SwiGLU |\n| 2.1.4 æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ– | p23 | Section 5: LayerNorm â†’ RMSNorm |\n| 2.1.5 ç¼–ç å™¨å’Œè§£ç å™¨ç»“æ„ | p25 | Section 1: Encoder-Decoder vs Decoder-Only |\n| 2.2.1 è‡ªç›‘ç£é¢„è®­ç»ƒ | p30 | Section 2: NTP è®­ç»ƒç›®æ ‡ |\n| 2.2.3 é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å®è·µ | p32 | Section 4: å­—ç¬¦çº§è¯­è¨€æ¨¡å‹è®­ç»ƒ |\n| 2.3.1 LLaMA çš„æ¨¡å‹ç»“æ„ | p41 | Section 8: MiniLLaMA æ•´åˆ |\n| 2.3.2 æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ– | p49 | Section 7: GQA + Section 11.5: MLA |\n| 2.4 æ··åˆä¸“å®¶æ¨¡å‹ | p57 | Section 12 å¯¹æ¯”è¡¨ï¼ˆMoE ç•™åˆ° Step 4ï¼‰|\n\n---\n\n## Part 1: Decoder-Only åŸºç¡€"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ä» Encoder-Decoder åˆ° Decoder-Only\n",
    "\n",
    "**Step 2 çš„å®Œæ•´ Transformerï¼š**\n",
    "```\n",
    "Encoder (Self-Attention + FFN) Ã— N\n",
    "    â†“ memory\n",
    "Decoder (Masked Self-Attn + Cross-Attn + FFN) Ã— N\n",
    "    â†“\n",
    "Output\n",
    "```\n",
    "\n",
    "**GPT çš„ Decoder-Onlyï¼š**\n",
    "```\n",
    "Decoder (Masked Self-Attention + FFN) Ã— N\n",
    "    â†“\n",
    "Output\n",
    "```\n",
    "\n",
    "| åŒºåˆ« | Encoder-Decoder | Decoder-Only (GPT) |\n",
    "|------|----------------|---------------------|\n",
    "| Encoder | âœ… æœ‰ | âŒ å»æ‰ |\n",
    "| Cross-Attention | âœ… æœ‰ | âŒ å»æ‰ |\n",
    "| Self-Attention | åŒå‘(Encoder) + å› æœ(Decoder) | åªæœ‰å› æœ(Causal) |\n",
    "| å…¸å‹ä»»åŠ¡ | ç¿»è¯‘ã€æ‘˜è¦ | æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ |\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šåªä¿ç•™ Causal Masked Self-Attention + FFNï¼Œä¸€ä¸ª token åªèƒ½çœ‹åˆ°å®ƒå‰é¢çš„ tokenã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NTP: Next Token Prediction\n",
    "\n",
    "**æ ¸å¿ƒæ¦‚å¿µ**ï¼šè¾“å…¥å³ç§»ä¸€ä½å°±æ˜¯æ ‡ç­¾\n",
    "\n",
    "```\n",
    "è¾“å…¥: [A, B, C, D]    â†’ æ¨¡å‹é¢„æµ‹æ¯ä¸ªä½ç½®çš„\"ä¸‹ä¸€ä¸ª token\"\n",
    "æ ‡ç­¾: [B, C, D, E]    â†’ å°±æ˜¯è¾“å…¥å¾€åç§»ä¸€ä½\n",
    "\n",
    "Loss = CrossEntropy(é¢„æµ‹, æ ‡ç­¾)  å¯¹æ¯ä¸ªä½ç½®éƒ½ç®—ï¼Œç„¶åå–å¹³å‡\n",
    "```\n",
    "\n",
    "è¿™å’Œ Step 1 çš„ CE Loss å®Œå…¨ä¸€æ ·ï¼Œåªæ˜¯ç°åœ¨å¯¹åºåˆ—ä¸­**æ¯ä¸ªä½ç½®**éƒ½ç®— lossã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥: [0, 1, 2, 3]  (A=0, B=1, C=2, D=3)\n",
      "æ ‡ç­¾: [1, 2, 3, 4]  (B=1, C=2, D=3, E=4)\n",
      "\n",
      "æ¯ä¸ªä½ç½®çš„ä»»åŠ¡:\n",
      "  ä½ç½® 0: çœ‹åˆ° [0] â†’ é¢„æµ‹ 1\n",
      "  ä½ç½® 1: çœ‹åˆ° [0, 1] â†’ é¢„æµ‹ 2\n",
      "  ä½ç½® 2: çœ‹åˆ° [0, 1, 2] â†’ é¢„æµ‹ 3\n",
      "  ä½ç½® 3: çœ‹åˆ° [0, 1, 2, 3] â†’ é¢„æµ‹ 4\n",
      "\n",
      "CE Loss = 1.9544\n",
      "ç†è®ºéšæœº loss = -ln(1/5) = 1.6094\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "# === NTP: Next Token Prediction æ¼”ç¤º ===\n",
    "# å‡è®¾ vocab: A=0, B=1, C=2, D=3, E=4\n",
    "sequence = torch.tensor([0, 1, 2, 3, 4])  # [A, B, C, D, E]\n",
    "\n",
    "# è¾“å…¥ = å‰ n-1 ä¸ª tokenï¼Œæ ‡ç­¾ = å n-1 ä¸ª token\n",
    "inputs = sequence[:-1]   # [A, B, C, D]\n",
    "labels = sequence[1:]    # [B, C, D, E]\n",
    "\n",
    "print(f\"è¾“å…¥: {inputs.tolist()}  (A=0, B=1, C=2, D=3)\")\n",
    "print(f\"æ ‡ç­¾: {labels.tolist()}  (B=1, C=2, D=3, E=4)\")\n",
    "print(f\"\\næ¯ä¸ªä½ç½®çš„ä»»åŠ¡:\")\n",
    "for i in range(len(inputs)):\n",
    "    print(f\"  ä½ç½® {i}: çœ‹åˆ° {inputs[:i+1].tolist()} â†’ é¢„æµ‹ {labels[i].item()}\")\n",
    "\n",
    "# æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º logits: (seq_len, vocab_size)\n",
    "vocab_size = 5\n",
    "logits = torch.randn(4, vocab_size)\n",
    "\n",
    "# CE Loss\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "print(f\"\\nCE Loss = {loss.item():.4f}\")\n",
    "print(f\"ç†è®ºéšæœº loss = -ln(1/{vocab_size}) = {math.log(vocab_size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ‰‹å†™ Mini GPT æ¨¡å‹\n",
    "\n",
    "å¤ç”¨ Step 2 çš„ç»„ä»¶æ€è·¯ï¼Œä½†ç®€åŒ–ä¸º Decoder-Onlyï¼š\n",
    "- âœ… Causal Maskï¼ˆStep 2 ç¬¬5èŠ‚ï¼‰\n",
    "- âœ… RoPEï¼ˆStep 2 ç¬¬12èŠ‚ï¼‰\n",
    "- âŒ å»æ‰ Encoderã€Cross-Attention\n",
    "- â• åŠ ä¸Š Token Embedding + è¾“å‡ºæŠ•å½±å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥ shape: torch.Size([2, 10])\n",
      "è¾“å‡º logits shape: torch.Size([2, 10, 100])\n",
      "æ¨¡å‹å‚æ•°é‡: 111,744\n"
     ]
    }
   ],
   "source": [
    "# === åŸºç¡€ç»„ä»¶ ===\n",
    "\n",
    "def precompute_rope_freqs(dim, max_seq_len, base=10000.0):\n",
    "    \"\"\"é¢„è®¡ç®— RoPE é¢‘ç‡ (å¤ç”¨ Step 2 ç¬¬12èŠ‚)\"\"\"\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    positions = torch.arange(max_seq_len).float()\n",
    "    angles = torch.outer(positions, freqs)  # (seq_len, dim/2)\n",
    "    return torch.cos(angles), torch.sin(angles)\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    \"\"\"å¯¹ x åº”ç”¨ RoPE: x shape = (B, n_heads, T, head_dim)\"\"\"\n",
    "    seq_len = x.shape[-2]\n",
    "    cos = cos[:seq_len].unsqueeze(0).unsqueeze(0)  # (1,1,T,dim/2)\n",
    "    sin = sin[:seq_len].unsqueeze(0).unsqueeze(0)\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"å¤šå¤´å› æœè‡ªæ³¨æ„åŠ› + RoPE\"\"\"\n",
    "    def __init__(self, model_dim, n_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = model_dim // n_heads\n",
    "        self.Wq = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.Wk = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.Wv = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.Wo = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        cos, sin = precompute_rope_freqs(self.head_dim, max_seq_len)\n",
    "        self.register_buffer('cos', cos)\n",
    "        self.register_buffer('sin', sin)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        q = self.Wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.Wk(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.Wv(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        q = apply_rope(q, self.cos, self.sin)\n",
    "        k = apply_rope(k, self.cos, self.sin)\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.Wo(out)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"å‰é¦ˆç½‘ç»œ (ReLU ç‰ˆæœ¬ï¼Œåé¢ä¼šå‡çº§ä¸º SwiGLU)\"\"\"\n",
    "    def __init__(self, model_dim, ff_dim=None):\n",
    "        super().__init__()\n",
    "        ff_dim = ff_dim or model_dim * 4\n",
    "        self.w1 = nn.Linear(model_dim, ff_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ff_dim, model_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.relu(self.w1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Decoder Block: Pre-Norm â†’ Attention â†’ Residual â†’ Pre-Norm â†’ FFN â†’ Residual\"\"\"\n",
    "    def __init__(self, model_dim, n_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.attn = MultiHeadAttention(model_dim, n_heads, max_seq_len)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.ffn = FFN(model_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"Mini GPT: Token Embedding + N Ã— DecoderBlock + LM Head\"\"\"\n",
    "    def __init__(self, vocab_size, model_dim=64, n_heads=4, n_layers=2, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, model_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(model_dim, n_heads, max_seq_len) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.lm_head = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_emb(idx)  # (B, T, D)\n",
    "        mask = torch.tril(torch.ones(T, T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "# æµ‹è¯•\n",
    "model = MiniGPT(vocab_size=100, model_dim=64, n_heads=4, n_layers=2)\n",
    "x = torch.randint(0, 100, (2, 10))\n",
    "logits, _ = model(x)\n",
    "print(f\"è¾“å…¥ shape: {x.shape}\")\n",
    "print(f\"è¾“å‡º logits shape: {logits.shape}\")  # (2, 10, 100)\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è®­ç»ƒä¸€ä¸ªå­—ç¬¦çº§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "ç”¨ä¸€å°æ®µä¸­æ–‡æ–‡æœ¬è®­ç»ƒ MiniGPTï¼Œç›´è§‚æ„Ÿå—ï¼š\n",
    "1. Loss ä¸‹é™è¿‡ç¨‹\n",
    "2. è‡ªå›å½’ç”Ÿæˆï¼ˆæ¨¡å‹\"ç»­å†™\"ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === å­—ç¬¦çº§è¯­è¨€æ¨¡å‹è®­ç»ƒ ===\n",
    "text = \"å¤©åœ°ç„é»„å®‡å®™æ´ªè’æ—¥æœˆç›ˆæ˜ƒè¾°å®¿åˆ—å¼ å¯’æ¥æš‘å¾€ç§‹æ”¶å†¬è—é—°ä½™æˆå²å¾‹å•è°ƒé˜³äº‘è…¾è‡´é›¨éœ²ç»“ä¸ºéœœ\"\n",
    "\n",
    "# æ„å»ºå­—ç¬¦è¯è¡¨\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "data = torch.tensor([char2idx[c] for c in text])\n",
    "\n",
    "print(f\"æ–‡æœ¬é•¿åº¦: {len(text)}, è¯è¡¨å¤§å°: {vocab_size}\")\n",
    "print(f\"è¯è¡¨: {''.join(chars)}\")\n",
    "\n",
    "# æ„å»ºè®­ç»ƒæ•°æ®: æ»‘åŠ¨çª—å£\n",
    "seq_len = 16\n",
    "xs, ys = [], []\n",
    "for i in range(len(data) - seq_len):\n",
    "    xs.append(data[i:i+seq_len])\n",
    "    ys.append(data[i+1:i+seq_len+1])\n",
    "xs, ys = torch.stack(xs), torch.stack(ys)\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {xs.shape[0]}, åºåˆ—é•¿åº¦: {seq_len}\")\n",
    "\n",
    "# è®­ç»ƒ\n",
    "model = MiniGPT(vocab_size=vocab_size, model_dim=64, n_heads=4, n_layers=2, max_seq_len=seq_len+1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    logits, loss = model(xs, ys)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# è‡ªå›å½’ç”Ÿæˆ\n",
    "def generate_greedy(model, start_ids, max_new=20):\n",
    "    model.eval()\n",
    "    idx = start_ids.unsqueeze(0)  # (1, T)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new):\n",
    "            logits, _ = model(idx[:, -model.max_seq_len:])\n",
    "            next_id = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n",
    "print(f\"\\n=== ç”Ÿæˆæµ‹è¯• ===\")\n",
    "for start in [\"å¤©åœ°\", \"å¯’æ¥\", \"äº‘è…¾\"]:\n",
    "    ids = torch.tensor([char2idx[c] for c in start])\n",
    "    out = generate_greedy(model, ids)\n",
    "    print(f\"  '{start}' â†’ {''.join(idx2char.get(i, '?') for i in out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 å°ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¦ç‚¹ |\n",
    "|------|------|\n",
    "| Decoder-Only | å»æ‰ Encoder + Cross-Attentionï¼Œåªä¿ç•™ Causal Self-Attention + FFN |\n",
    "| NTP | `input[:-1]` é¢„æµ‹ `input[1:]`ï¼Œæ¯ä¸ªä½ç½®éƒ½ç®— CE Loss |\n",
    "| Causal Mask | ä¸‹ä¸‰è§’çŸ©é˜µï¼Œä¿è¯ token åªçœ‹åˆ°å‰é¢çš„ |\n",
    "| RoPE | æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä¸éœ€è¦å­¦ä¹ ï¼Œå¤–æ¨æ€§å¥½ |\n",
    "| è‡ªå›å½’ç”Ÿæˆ | å¾ªç¯ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token â†’ æ‹¼æ¥ â†’ å†é¢„æµ‹ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LLaMA æ”¹åŠ¨\n",
    "\n",
    "ç°ä»£ LLMï¼ˆLLaMAã€Qwenã€DeepSeekï¼‰å¯¹åŸç‰ˆ Transformer åšäº†å‡ ä¸ªå…³é”®æ”¹åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RMSNorm æ›¿ä»£ LayerNorm\n",
    "\n",
    "**ä¸ºä»€ä¹ˆï¼Ÿ** RMSNorm å»æ‰äº†å‡å€¼è®¡ç®—ï¼Œåªä¿ç•™æ–¹å·®å½’ä¸€åŒ–ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œæ•ˆæœç›¸å½“ã€‚\n",
    "\n",
    "| | LayerNorm | RMSNorm |\n",
    "|---|-----------|---------|\n",
    "| å…¬å¼ | $(x - \\mu) / \\sigma \\cdot \\gamma + \\beta$ | $x / \\text{RMS}(x) \\cdot \\gamma$ |\n",
    "| å‡å€¼ | âœ… éœ€è¦è®¡ç®— | âŒ ä¸éœ€è¦ |\n",
    "| åç½® | âœ… æœ‰ Î² | âŒ æ²¡æœ‰ |\n",
    "| é€Ÿåº¦ | è¾ƒæ…¢ | æ›´å¿« |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RMSNorm vs LayerNorm ===\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization (LLaMA/Qwen ä½¿ç”¨)\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "# å¯¹æ¯”\n",
    "x = torch.randn(2, 5, 64)\n",
    "ln = nn.LayerNorm(64)\n",
    "rmsn = RMSNorm(64)\n",
    "\n",
    "out_ln = ln(x)\n",
    "out_rms = rmsn(x)\n",
    "print(f\"è¾“å…¥ shape: {x.shape}\")\n",
    "print(f\"LayerNorm è¾“å‡º mean: {out_ln.mean():.6f}, std: {out_ln.std():.4f}\")\n",
    "print(f\"RMSNorm   è¾“å‡º mean: {out_rms.mean():.6f}, std: {out_rms.std():.4f}\")\n",
    "print(f\"\\nLayerNorm å‚æ•°: weight + bias = {sum(p.numel() for p in ln.parameters())}\")\n",
    "print(f\"RMSNorm   å‚æ•°: weight only  = {sum(p.numel() for p in rmsn.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SwiGLU æ›¿ä»£ ReLU FFN\n",
    "\n",
    "**ä¸ºä»€ä¹ˆï¼Ÿ** SwiGLU æä¾›æ›´å¼ºçš„éçº¿æ€§èƒ½åŠ›ï¼ŒLLaMA / Qwen / DeepSeek éƒ½åœ¨ç”¨ã€‚\n",
    "\n",
    "| | ReLU FFN | SwiGLU FFN |\n",
    "|---|---------|------------|\n",
    "| å…¬å¼ | ReLU(xWâ‚)Wâ‚‚ | (Swish(xWâ‚) âŠ™ xWâ‚ƒ)Wâ‚‚ |\n",
    "| é—¨æ§ | âŒ | âœ… æœ‰é—¨æ§æœºåˆ¶ |\n",
    "| éšè—å±‚ | 4d | 8d/3ï¼ˆè°ƒæ•´åå‚æ•°é‡ç›¸å½“ï¼‰|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SwiGLU FFN ===\n",
    "\n",
    "class SwiGLU_FFN(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network (LLaMA é£æ ¼)\"\"\"\n",
    "    def __init__(self, model_dim, ff_dim=None):\n",
    "        super().__init__()\n",
    "        ff_dim = ff_dim or int(model_dim * 8 / 3)\n",
    "        self.w1 = nn.Linear(model_dim, ff_dim, bias=False)  # gate\n",
    "        self.w3 = nn.Linear(model_dim, ff_dim, bias=False)  # up\n",
    "        self.w2 = nn.Linear(ff_dim, model_dim, bias=False)  # down\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "# å¯¹æ¯”å‚æ•°é‡\n",
    "d = 64\n",
    "relu_ffn = FFN(d)\n",
    "swiglu_ffn = SwiGLU_FFN(d)\n",
    "print(f\"ReLU FFN   å‚æ•°é‡: {sum(p.numel() for p in relu_ffn.parameters()):,}\")\n",
    "print(f\"SwiGLU FFN å‚æ•°é‡: {sum(p.numel() for p in swiglu_ffn.parameters()):,}\")\n",
    "\n",
    "x = torch.randn(2, 5, d)\n",
    "print(f\"\\nReLU FFN   è¾“å‡º shape: {relu_ffn(x).shape}\")\n",
    "print(f\"SwiGLU FFN è¾“å‡º shape: {swiglu_ffn(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GQA: Grouped Query Attention\n",
    "\n",
    "**æ¼”è¿›è·¯çº¿**ï¼šMHA â†’ MQA â†’ GQA\n",
    "\n",
    "| | MHA | MQA | GQA |\n",
    "|---|-----|-----|-----|\n",
    "| Q å¤´æ•° | H | H | H |\n",
    "| K/V å¤´æ•° | H | 1 | G (1 < G < H) |\n",
    "| å‚æ•°é‡ | æœ€å¤š | æœ€å°‘ | ä¸­é—´ |\n",
    "| æ•ˆæœ | æœ€å¥½ | ç•¥å·® | æ¥è¿‘ MHA |\n",
    "| ä»£è¡¨ | GPT-2 | PaLM | LLaMA-2/3, Qwen |\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šå¤šä¸ª Q å¤´å…±äº«åŒä¸€ç»„ K/V å¤´ï¼Œå‡å°‘ KV Cache æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GQA: Grouped Query Attention ===\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"GQA: n_kv_heads ç»„ K/Vï¼Œæ¯ç»„è¢« n_heads/n_kv_heads ä¸ª Q å¤´å…±äº«\"\"\"\n",
    "    def __init__(self, model_dim, n_heads, n_kv_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        assert n_heads % n_kv_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_heads // n_kv_heads\n",
    "        self.head_dim = model_dim // n_heads\n",
    "\n",
    "        self.Wq = nn.Linear(model_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(model_dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(model_dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        cos, sin = precompute_rope_freqs(self.head_dim, max_seq_len)\n",
    "        self.register_buffer('cos', cos)\n",
    "        self.register_buffer('sin', sin)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.Wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.Wk(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.Wv(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        q = apply_rope(q, self.cos, self.sin)\n",
    "        k = apply_rope(k, self.cos, self.sin)\n",
    "        # æ‰©å±• KV: (B, n_kv_heads, T, D) â†’ (B, n_heads, T, D)\n",
    "        k = k.repeat_interleave(self.n_rep, dim=1)\n",
    "        v = v.repeat_interleave(self.n_rep, dim=1)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        out = torch.matmul(F.softmax(scores, dim=-1), v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.Wo(out)\n",
    "\n",
    "# å¯¹æ¯”å‚æ•°é‡\n",
    "d, h = 64, 8\n",
    "mha = MultiHeadAttention(d, h)\n",
    "gqa = GroupedQueryAttention(d, n_heads=h, n_kv_heads=2)\n",
    "print(f\"MHA (8Q, 8KV) å‚æ•°é‡: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "print(f\"GQA (8Q, 2KV) å‚æ•°é‡: {sum(p.numel() for p in gqa.parameters()):,}\")\n",
    "print(f\"KV å‚æ•°èŠ‚çœ: {1 - sum(p.numel() for p in gqa.parameters()) / sum(p.numel() for p in mha.parameters()):.1%}\")\n",
    "\n",
    "x = torch.randn(2, 10, d)\n",
    "mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0)\n",
    "print(f\"\\nGQA è¾“å‡º shape: {gqa(x, mask).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å‡çº§ç‰ˆ Mini GPTï¼ˆLLaMA é£æ ¼ï¼‰\n",
    "\n",
    "æŠŠ Section 5/6/7 çš„æ”¹åŠ¨æ•´åˆï¼šRMSNorm + SwiGLU + GQA + Pre-Norm\n",
    "\n",
    "| ç»„ä»¶ | åŸç‰ˆ GPT | LLaMA é£æ ¼ |\n",
    "|------|---------|------------|\n",
    "| Norm | LayerNorm | RMSNorm |\n",
    "| FFN | ReLU, 4d | SwiGLU, 8d/3 |\n",
    "| Attention | MHA | GQA |\n",
    "| Norm ä½ç½® | Post-Norm æˆ– Pre-Norm | Pre-Norm |\n",
    "| ä½ç½®ç¼–ç  | å­¦ä¹ å¼ / Sinusoidal | RoPE |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLaMA é£æ ¼ Mini GPT ===\n",
    "\n",
    "class LLaMABlock(nn.Module):\n",
    "    \"\"\"LLaMA Decoder Block: RMSNorm + GQA + SwiGLU\"\"\"\n",
    "    def __init__(self, model_dim, n_heads, n_kv_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(model_dim)\n",
    "        self.attn = GroupedQueryAttention(model_dim, n_heads, n_kv_heads, max_seq_len)\n",
    "        self.norm2 = RMSNorm(model_dim)\n",
    "        self.ffn = SwiGLU_FFN(model_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MiniLLaMA(nn.Module):\n",
    "    \"\"\"LLaMA é£æ ¼çš„ Mini GPT\"\"\"\n",
    "    def __init__(self, vocab_size, model_dim=64, n_heads=8, n_kv_heads=2,\n",
    "                 n_layers=2, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, model_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            LLaMABlock(model_dim, n_heads, n_kv_heads, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(model_dim)\n",
    "        self.lm_head = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_emb(idx)\n",
    "        mask = torch.tril(torch.ones(T, T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        logits = self.lm_head(self.norm(x))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "# å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹\n",
    "v = 100\n",
    "gpt = MiniGPT(v, model_dim=64, n_heads=4, n_layers=2)\n",
    "llama = MiniLLaMA(v, model_dim=64, n_heads=8, n_kv_heads=2, n_layers=2)\n",
    "print(f\"åŸç‰ˆ MiniGPT  å‚æ•°é‡: {sum(p.numel() for p in gpt.parameters()):,}\")\n",
    "print(f\"MiniLLaMA     å‚æ•°é‡: {sum(p.numel() for p in llama.parameters()):,}\")\n",
    "\n",
    "x = torch.randint(0, v, (2, 10))\n",
    "print(f\"\\nMiniLLaMA è¾“å‡º shape: {llama(x)[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ç”Ÿæˆä¸æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è‡ªå›å½’ç”Ÿæˆç­–ç•¥\n",
    "\n",
    "4 ç§å¸¸è§çš„ç”Ÿæˆæ–¹å¼ï¼š\n",
    "\n",
    "| ç­–ç•¥ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|----------|\n",
    "| Greedy | æ¯æ¬¡é€‰æ¦‚ç‡æœ€å¤§çš„ | ç¡®å®šæ€§ä»»åŠ¡ï¼ˆä»£ç ã€æ•°å­¦ï¼‰|\n",
    "| Temperature | è°ƒèŠ‚åˆ†å¸ƒé”åº¦ | é€šç”¨ |\n",
    "| Top-K | åªä»å‰ K ä¸ªå€™é€‰ä¸­é‡‡æ · | å¹³è¡¡å¤šæ ·æ€§å’Œè´¨é‡ |\n",
    "| Top-P (Nucleus) | ä»ç´¯ç§¯æ¦‚ç‡è¾¾ P çš„å€™é€‰ä¸­é‡‡æ · | è‡ªé€‚åº”å€™é€‰æ•°é‡ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4 ç§ç”Ÿæˆç­–ç•¥ ===\n",
    "\n",
    "# å…ˆè®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ç”¨äºæ¼”ç¤º\n",
    "text = \"å¤©åœ°ç„é»„å®‡å®™æ´ªè’æ—¥æœˆç›ˆæ˜ƒè¾°å®¿åˆ—å¼ å¯’æ¥æš‘å¾€ç§‹æ”¶å†¬è—é—°ä½™æˆå²å¾‹å•è°ƒé˜³äº‘è…¾è‡´é›¨éœ²ç»“ä¸ºéœœé‡‘ç”Ÿä¸½æ°´ç‰å‡ºæ˜†å†ˆå‰‘å·å·¨é˜™ç ç§°å¤œå…‰æœçææŸ°èœé‡èŠ¥å§œæµ·å’¸æ²³æ·¡é³æ½œç¾½ç¿”\"\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "c2i = {c: i for i, c in enumerate(chars)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "data = torch.tensor([c2i[c] for c in text])\n",
    "\n",
    "seq_len = 16\n",
    "xs = torch.stack([data[i:i+seq_len] for i in range(len(data)-seq_len)])\n",
    "ys = torch.stack([data[i+1:i+seq_len+1] for i in range(len(data)-seq_len)])\n",
    "\n",
    "model = MiniLLaMA(vocab_size, model_dim=64, n_heads=8, n_kv_heads=2, n_layers=2, max_seq_len=seq_len+1)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for ep in range(300):\n",
    "    _, loss = model(xs, ys)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if (ep+1) % 100 == 0:\n",
    "        print(f\"Epoch {ep+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ç”Ÿæˆå‡½æ•°\n",
    "def generate(model, start, max_new=20, strategy='greedy', temperature=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[c2i[c] for c in start]])\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new):\n",
    "            logits, _ = model(idx[:, -model.max_seq_len:])\n",
    "            logits = logits[:, -1, :] / temperature  # temperature ç¼©æ”¾\n",
    "\n",
    "            if top_k > 0:  # Top-K: åªä¿ç•™å‰ K ä¸ª\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, -1:]] = float('-inf')\n",
    "\n",
    "            if top_p > 0:  # Top-P: ç´¯ç§¯æ¦‚ç‡æˆªæ–­\n",
    "                sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "                cumprobs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                mask = cumprobs - F.softmax(sorted_logits, dim=-1) >= top_p\n",
    "                sorted_logits[mask] = float('-inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_idx, sorted_logits)\n",
    "\n",
    "            if strategy == 'greedy':\n",
    "                next_id = logits.argmax(dim=-1, keepdim=True)\n",
    "            else:\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_id = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "    return ''.join(i2c.get(i, '?') for i in idx[0].tolist())\n",
    "\n",
    "# å¯¹æ¯” 4 ç§ç­–ç•¥\n",
    "start = \"å¤©åœ°\"\n",
    "print(f\"=== ä» '{start}' å¼€å§‹ç”Ÿæˆ ===\\n\")\n",
    "print(f\"Greedy:      {generate(model, start, strategy='greedy')}\")\n",
    "print(f\"Temp=0.5:    {generate(model, start, strategy='sample', temperature=0.5)}\")\n",
    "print(f\"Temp=1.5:    {generate(model, start, strategy='sample', temperature=1.5)}\")\n",
    "print(f\"Top-K=5:     {generate(model, start, strategy='sample', top_k=5)}\")\n",
    "print(f\"Top-P=0.9:   {generate(model, start, strategy='sample', top_p=0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. KV Cache\n",
    "\n",
    "**é—®é¢˜**ï¼šè‡ªå›å½’ç”Ÿæˆæ—¶ï¼Œæ¯ç”Ÿæˆä¸€ä¸ª token éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰å†å² token çš„ K/Vï¼Œæµªè´¹ç®—åŠ›ã€‚\n",
    "\n",
    "**è§£å†³**ï¼šç¼“å­˜å·²è®¡ç®—çš„ K/Vï¼Œæ¯æ­¥åªè®¡ç®—æ–° token çš„ Q/K/Vã€‚\n",
    "\n",
    "```\n",
    "æ—  Cache: ç”Ÿæˆç¬¬ n ä¸ª token æ—¶ï¼Œè®¡ç®— n æ¬¡ K/V  â†’ O(nÂ²) æ€»è®¡ç®—é‡\n",
    "æœ‰ Cache: ç”Ÿæˆç¬¬ n ä¸ª token æ—¶ï¼Œåªç®— 1 æ¬¡ K/V  â†’ O(n) æ€»è®¡ç®—é‡\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KV Cache å®ç° ===\n",
    "\n",
    "class GQA_WithCache(nn.Module):\n",
    "    \"\"\"å¸¦ KV Cache çš„ GQA\"\"\"\n",
    "    def __init__(self, model_dim, n_heads, n_kv_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_rep = n_heads // n_kv_heads\n",
    "        self.head_dim = model_dim // n_heads\n",
    "        self.Wq = nn.Linear(model_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(model_dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(model_dim, n_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        cos, sin = precompute_rope_freqs(self.head_dim, max_seq_len)\n",
    "        self.register_buffer('cos', cos)\n",
    "        self.register_buffer('sin', sin)\n",
    "\n",
    "    def forward(self, x, start_pos, mask=None, kv_cache=None):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.Wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.Wk(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.Wv(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # RoPE: ç”¨ start_pos åç§»ï¼Œç¡®ä¿ä½ç½®æ­£ç¡®\n",
    "        cos = self.cos[start_pos:start_pos+T].unsqueeze(0).unsqueeze(0)\n",
    "        sin = self.sin[start_pos:start_pos+T].unsqueeze(0).unsqueeze(0)\n",
    "        q1, q2 = q[..., :self.head_dim//2], q[..., self.head_dim//2:]\n",
    "        q = torch.cat([q1*cos - q2*sin, q1*sin + q2*cos], dim=-1)\n",
    "        k1, k2 = k[..., :self.head_dim//2], k[..., self.head_dim//2:]\n",
    "        k = torch.cat([k1*cos - k2*sin, k1*sin + k2*cos], dim=-1)\n",
    "\n",
    "        # KV Cache: æ‹¼æ¥å†å²\n",
    "        if kv_cache is not None:\n",
    "            k = torch.cat([kv_cache[0], k], dim=2)\n",
    "            v = torch.cat([kv_cache[1], v], dim=2)\n",
    "        new_cache = (k, v)\n",
    "\n",
    "        # æ‰©å±• KV å¤´\n",
    "        k_exp = k.repeat_interleave(self.n_rep, dim=1)\n",
    "        v_exp = v.repeat_interleave(self.n_rep, dim=1)\n",
    "\n",
    "        scores = torch.matmul(q, k_exp.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        out = torch.matmul(F.softmax(scores, dim=-1), v_exp)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.Wo(out), new_cache\n",
    "\n",
    "class LLaMABlock_WithCache(nn.Module):\n",
    "    def __init__(self, model_dim, n_heads, n_kv_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(model_dim)\n",
    "        self.attn = GQA_WithCache(model_dim, n_heads, n_kv_heads, max_seq_len)\n",
    "        self.norm2 = RMSNorm(model_dim)\n",
    "        self.ffn = SwiGLU_FFN(model_dim)\n",
    "\n",
    "    def forward(self, x, start_pos, mask=None, kv_cache=None):\n",
    "        h, new_cache = self.attn(self.norm1(x), start_pos, mask, kv_cache)\n",
    "        x = x + h\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x, new_cache\n",
    "\n",
    "class MiniLLaMA_WithCache(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim=64, n_heads=8, n_kv_heads=2,\n",
    "                 n_layers=2, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, model_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            LLaMABlock_WithCache(model_dim, n_heads, n_kv_heads, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(model_dim)\n",
    "        self.lm_head = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, start_pos=0, kv_caches=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok_emb(idx)\n",
    "        # Prefill ç”¨å› æœ maskï¼Œdecode é˜¶æ®µä¸éœ€è¦ maskï¼ˆåªæœ‰1ä¸ªtokenï¼Œèƒ½çœ‹åˆ°æ‰€æœ‰cacheï¼‰\n",
    "        if T > 1:\n",
    "            mask = torch.tril(torch.ones(T, T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            mask = None\n",
    "        new_caches = []\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            cache = kv_caches[i] if kv_caches else None\n",
    "            x, new_cache = block(x, start_pos, mask, cache)\n",
    "            new_caches.append(new_cache)\n",
    "        logits = self.lm_head(self.norm(x))\n",
    "        return logits, new_caches\n",
    "\n",
    "# === é€Ÿåº¦å¯¹æ¯” ===\n",
    "def generate_no_cache(model_nc, start_ids, n=50):\n",
    "    idx = start_ids.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n):\n",
    "            logits, _ = model_nc(idx)\n",
    "            idx = torch.cat([idx, logits[:, -1:].argmax(-1)], dim=1)\n",
    "    return idx[0]\n",
    "\n",
    "def generate_with_cache(model_c, start_ids, n=50):\n",
    "    # Prefill\n",
    "    idx = start_ids.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits, caches = model_c(idx, start_pos=0)\n",
    "        next_id = logits[:, -1:].argmax(-1)\n",
    "        all_ids = [idx, next_id]\n",
    "        pos = idx.shape[1]\n",
    "        for _ in range(n - 1):\n",
    "            logits, caches = model_c(next_id, start_pos=pos, kv_caches=caches)\n",
    "            next_id = logits[:, -1:].argmax(-1)\n",
    "            all_ids.append(next_id)\n",
    "            pos += 1\n",
    "    return torch.cat(all_ids, dim=1)[0]\n",
    "\n",
    "# æ„å»ºä¸¤ä¸ªæ¨¡å‹ï¼ˆå…±äº«æƒé‡æ¥å…¬å¹³å¯¹æ¯”ï¼‰\n",
    "v_size = 200\n",
    "m_nc = MiniLLaMA(v_size, model_dim=64, n_heads=8, n_kv_heads=2, n_layers=4, max_seq_len=256)\n",
    "m_c = MiniLLaMA_WithCache(v_size, model_dim=64, n_heads=8, n_kv_heads=2, n_layers=4, max_seq_len=256)\n",
    "\n",
    "# å¤åˆ¶æƒé‡\n",
    "m_c.tok_emb.load_state_dict(m_nc.tok_emb.state_dict())\n",
    "m_c.norm.load_state_dict(m_nc.norm.state_dict())\n",
    "m_c.lm_head.load_state_dict(m_nc.lm_head.state_dict())\n",
    "for i in range(4):\n",
    "    m_c.blocks[i].norm1.load_state_dict(m_nc.blocks[i].norm1.state_dict())\n",
    "    m_c.blocks[i].norm2.load_state_dict(m_nc.blocks[i].norm2.state_dict())\n",
    "    m_c.blocks[i].ffn.load_state_dict(m_nc.blocks[i].ffn.state_dict())\n",
    "    m_c.blocks[i].attn.Wq.load_state_dict(m_nc.blocks[i].attn.Wq.state_dict())\n",
    "    m_c.blocks[i].attn.Wk.load_state_dict(m_nc.blocks[i].attn.Wk.state_dict())\n",
    "    m_c.blocks[i].attn.Wv.load_state_dict(m_nc.blocks[i].attn.Wv.state_dict())\n",
    "    m_c.blocks[i].attn.Wo.load_state_dict(m_nc.blocks[i].attn.Wo.state_dict())\n",
    "\n",
    "m_nc.eval(); m_c.eval()\n",
    "start = torch.randint(0, v_size, (5,))\n",
    "gen_len = 100\n",
    "\n",
    "t0 = time.time()\n",
    "out1 = generate_no_cache(m_nc, start, gen_len)\n",
    "t_no = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "out2 = generate_with_cache(m_c, start, gen_len)\n",
    "t_yes = time.time() - t0\n",
    "\n",
    "print(f\"ç”Ÿæˆ {gen_len} tokens:\")\n",
    "print(f\"  æ—  Cache: {t_no:.4f}s\")\n",
    "print(f\"  æœ‰ Cache: {t_yes:.4f}s\")\n",
    "print(f\"  åŠ é€Ÿæ¯”:   {t_no/t_yes:.2f}x\")\n",
    "print(f\"  ç»“æœä¸€è‡´: {torch.equal(out1, out2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: è¿æ¥ MiniOneRec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ä» GPT åˆ°æ¨èï¼šSID ç”Ÿæˆ\n",
    "\n",
    "MiniOneRec çš„æ ¸å¿ƒæ€æƒ³ï¼š**æŠŠæ¨èé—®é¢˜å˜æˆ\"ç”Ÿæˆä¸‹ä¸€ä¸ª token\"é—®é¢˜**ã€‚\n",
    "\n",
    "### æ™®é€š GPT vs MiniOneRec\n",
    "\n",
    "| | æ™®é€š GPT | MiniOneRec |\n",
    "|---|---------|------------|\n",
    "| è¯è¡¨ | è‡ªç„¶è¯­è¨€ tokenï¼ˆå‡ ä¸‡ä¸ªï¼‰| SID tokenï¼ˆå‡ ç™¾~å‡ åƒï¼‰|\n",
    "| è¾“å…¥ | æ–‡æœ¬ token åºåˆ— | ç”¨æˆ·å†å² SID åºåˆ— |\n",
    "| è¾“å‡º | ä¸‹ä¸€ä¸ªæ–‡å­— token | ä¸‹ä¸€ä¸ªç‰©å“ SID |\n",
    "| è®­ç»ƒ | NTP on æ–‡æœ¬ | NTP on SID åºåˆ—ï¼ˆSFTï¼‰|\n",
    "| ç”Ÿæˆ | è‡ªç”±ç”Ÿæˆ | **çº¦æŸè§£ç **ï¼ˆåªèƒ½ç”Ÿæˆåˆæ³• SIDï¼‰|\n",
    "\n",
    "### çº¦æŸè§£ç ï¼ˆLogitProcessorï¼‰\n",
    "\n",
    "```python\n",
    "# æ™®é€šç”Ÿæˆï¼šä»æ•´ä¸ªè¯è¡¨é‡‡æ ·\n",
    "next_token = sample(logits)  # å¯èƒ½ç”Ÿæˆä»»ä½• token\n",
    "\n",
    "# çº¦æŸè§£ç ï¼šåªå…è®¸åˆæ³•çš„ SID token\n",
    "valid_mask = get_valid_sids(prefix)  # æ ¹æ®å·²ç”Ÿæˆçš„å‰ç¼€ï¼Œç¡®å®šå“ªäº› SID åˆæ³•\n",
    "logits[~valid_mask] = -inf           # å±è”½éæ³• token\n",
    "next_token = sample(logits)          # åªèƒ½ç”Ÿæˆåˆæ³• SID\n",
    "```\n",
    "\n",
    "### MiniOneRec çš„ä¸‰é˜¶æ®µ\n",
    "\n",
    "```\n",
    "é˜¶æ®µ1: SID æ„å»º\n",
    "  ç‰©å“æ–‡æœ¬ â†’ Encoder â†’ åµŒå…¥ â†’ RQ-VAE â†’ SID (å¦‚ [12, 45, 78])\n",
    "\n",
    "é˜¶æ®µ2: SFTï¼ˆå°±æ˜¯æˆ‘ä»¬å­¦çš„ NTPï¼ï¼‰\n",
    "  ç”¨æˆ·å†å² [SID_1, SID_2, ...] â†’ GPT â†’ é¢„æµ‹ä¸‹ä¸€ä¸ª SID\n",
    "\n",
    "é˜¶æ®µ3: RLï¼ˆGRPOï¼‰\n",
    "  ç”¨ ranking reward è¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡\n",
    "```\n",
    "\n",
    "**å…³é”®æ´å¯Ÿ**ï¼šStep 3 å­¦çš„ MiniGPT è®­ç»ƒå’Œç”Ÿæˆï¼Œå°±æ˜¯ MiniOneRec SFT é˜¶æ®µçš„æ ¸å¿ƒï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11.5 DeepSeek MLA: Multi-Head Latent Attention\n\nGQA é€šè¿‡å‡å°‘ KV å¤´æ•°æ¥å‹ç¼© KV Cacheï¼Œä½† DeepSeek V2/V3 æå‡ºäº†ä¸€ç§æ›´æ¿€è¿›çš„æ–¹æ¡ˆï¼š**MLAï¼ˆMulti-head Latent Attentionï¼‰**ã€‚\n\n### æ ¸å¿ƒæ€æƒ³ï¼šä½ç§©å‹ç¼© KV\n\nGQA çš„æ€è·¯æ˜¯\"å°‘å­˜å‡ ä»½ KV\"ï¼ŒMLA çš„æ€è·¯æ˜¯\"æŠŠæ¯ä»½ KV å‹ç¼©åˆ°æ›´å°\"ã€‚\n\n```\nGQA:  ç¼“å­˜ n_kv_heads ç»„å®Œæ•´çš„ K, V     â†’ Cache å¤§å° = n_kv_heads Ã— head_dim Ã— 2\nMLA:  ç¼“å­˜ 1 ä¸ªä½ç§©å‹ç¼©å‘é‡ c_kv        â†’ Cache å¤§å° = d_cï¼ˆè¿œå°äºä¸Šé¢ï¼‰\n      æ¨ç†æ—¶ä» c_kv è¿˜åŸå‡ºæ‰€æœ‰å¤´çš„ K, V\n```\n\n### å¯¹æ¯”\n\n| | MHA | GQA | MLA |\n|---|-----|-----|-----|\n| KV Cache å­˜ä»€ä¹ˆ | æ‰€æœ‰å¤´çš„ K, V | n_kv_heads ç»„ K, V | **1 ä¸ªå‹ç¼©å‘é‡ c_kv** |\n| Cache å¤§å°/token | 2 Ã— H Ã— d_h | 2 Ã— G Ã— d_h | **d_c**ï¼ˆå¦‚ 512ï¼‰ |\n| è¿˜åŸæ–¹å¼ | ä¸éœ€è¦ | repeat K/V | çº¿æ€§æŠ•å½±è¿˜åŸ |\n| ä»£è¡¨æ¨¡å‹ | GPT-2 | LLaMA-2/3, Qwen | DeepSeek V2/V3 |\n\n### åŸç†å›¾è§£\n\n```\næ ‡å‡† MHA/GQA:\n  hidden â†’ Wk â†’ K (ç¼“å­˜è¿™ä¸ªï¼Œç»´åº¦å¤§)\n  hidden â†’ Wv â†’ V (ç¼“å­˜è¿™ä¸ªï¼Œç»´åº¦å¤§)\n\nMLA:\n  hidden â†’ W_dkv â†’ c_kv (å‹ç¼©ï¼Œç»´åº¦å°ï¼Œåªç¼“å­˜è¿™ä¸ªï¼)\n                    â†“\n              c_kv â†’ W_uk â†’ K (æ¨ç†æ—¶è¿˜åŸ)\n              c_kv â†’ W_uv â†’ V (æ¨ç†æ—¶è¿˜åŸ)\n```\n\n**æœ¬è´¨**ï¼šæŠŠ `hidden â†’ K` æ‹†æˆ `hidden â†’ c_kv â†’ K`ï¼Œä¸­é—´çš„ `c_kv` ç»´åº¦è¿œå°äº Kï¼Œæ‰€ä»¥ç¼“å­˜ `c_kv` æ›´çœæ˜¾å­˜ã€‚è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„**ä½ç§©åˆ†è§£**æŠ€å·§ï¼ˆç±»ä¼¼ LoRA çš„æ€æƒ³ï¼‰ã€‚\n\n### RoPE å…¼å®¹é—®é¢˜\n\nRoPE éœ€è¦ç›´æ¥ä½œç”¨åœ¨ K ä¸Šï¼Œä½† MLA çš„ K æ˜¯ä»å‹ç¼©å‘é‡è¿˜åŸçš„ï¼Œä¸¤è€…ä¸å…¼å®¹ã€‚DeepSeek çš„è§£å†³æ–¹æ¡ˆï¼š\n\n```\nK = [K_rope, K_nope]\n     â†‘          â†‘\n  å•ç‹¬æŠ•å½±     ä» c_kv è¿˜åŸ\n  (å¸¦ RoPE)   (ä¸å¸¦ RoPE)\n```\n\næŠŠ K æ‹†æˆä¸¤éƒ¨åˆ†ï¼šä¸€å°éƒ¨åˆ†ä¸“é—¨ç”¨äº RoPEï¼ˆå•ç‹¬ç¼“å­˜ï¼Œå¾ˆå°ï¼‰ï¼Œå…¶ä½™éƒ¨åˆ†èµ°ä½ç§©å‹ç¼©ã€‚\n\n### é¢è¯•ä¸€å¥è¯æ€»ç»“\n\n> **GQA æ˜¯\"å°‘å­˜å‡ ä»½ KV\"ï¼ŒMLA æ˜¯\"æŠŠ KV å‹ç¼©åå†å­˜\"ã€‚MLA ç”¨ä½ç§©æŠ•å½±å°† KV Cache å‹ç¼©åˆ°ä¸€ä¸ªå°å‘é‡ï¼Œæ¨ç†æ—¶å†è¿˜åŸï¼Œæ˜¾å­˜èŠ‚çœæ›´æ¿€è¿›ï¼Œä½†è®¡ç®—ç•¥å¤šã€‚**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. æ¨¡å‹å¯¹æ¯”æ€»ç»“\n\n| ç‰¹æ€§ | GPT-2 | LLaMA 2/3 | Qwen 2.5 | DeepSeek V3 |\n|------|-------|-----------|-----------|-------------|\n| Norm | LayerNorm | RMSNorm | RMSNorm | RMSNorm |\n| FFN | ReLU | SwiGLU | SwiGLU | SwiGLU + MoE |\n| Attention | MHA | GQA | GQA | MLA |\n| ä½ç½®ç¼–ç  | å­¦ä¹ å¼ | RoPE | RoPE | RoPE |\n| Norm ä½ç½® | Post-Norm | Pre-Norm | Pre-Norm | Pre-Norm |\n| Bias | æœ‰ | æ—  | æ—  | æ—  |\n\n### é¢è¯•è¦ç‚¹\n\n1. **ä¸ºä»€ä¹ˆ Decoder-Onlyï¼Ÿ** ç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œscaling law æ›´å¥½ï¼Œç”Ÿæˆä»»åŠ¡å¤©ç„¶é€‚é…\n2. **NTP ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ** é¢„æµ‹ä¸‹ä¸€ä¸ª token éšå«äº†å¯¹è¯­è¨€/åºåˆ—çš„æ·±å±‚ç†è§£\n3. **RMSNorm vs LayerNormï¼Ÿ** å»æ‰å‡å€¼è®¡ç®—ï¼Œæ›´å¿«ï¼Œæ•ˆæœç›¸å½“\n4. **SwiGLU vs ReLUï¼Ÿ** é—¨æ§æœºåˆ¶æä¾›æ›´å¼ºéçº¿æ€§ï¼Œå®éªŒæ•ˆæœæ›´å¥½\n5. **GQA çš„æ„ä¹‰ï¼Ÿ** å‡å°‘ KV Cache æ˜¾å­˜ï¼Œæ¨ç†æ›´å¿«ï¼Œæ•ˆæœæ¥è¿‘ MHA\n6. **KV Cache åŸç†ï¼Ÿ** ç¼“å­˜å†å² K/V é¿å…é‡å¤è®¡ç®—ï¼Œä» O(nÂ²) é™åˆ° O(n)\n7. **MLA vs GQAï¼Ÿ** GQA å‡å°‘ KV å¤´æ•°ï¼ŒMLA ç”¨ä½ç§©å‹ç¼©æŠŠ KV å‹åˆ°ä¸€ä¸ªå°å‘é‡å†ç¼“å­˜ï¼Œæ˜¾å­˜èŠ‚çœæ›´æ¿€è¿›ï¼›RoPE é€šè¿‡æ‹†åˆ† K ä¸º rope/nope ä¸¤éƒ¨åˆ†è§£å†³å…¼å®¹é—®é¢˜\n8. **çº¦æŸè§£ç ï¼Ÿ** åœ¨ logits ä¸Š mask éæ³• tokenï¼Œä¿è¯ç”Ÿæˆåˆæ³•è¾“å‡º"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ä¸‹ä¸€æ­¥\n",
    "\n",
    "- [ ] æŠŠ Step 3 å­¦ä¹ ç¬”è®°è¡¥åˆ° `study.md`\n",
    "- [ ] è¿›å…¥ Step 4ï¼šé˜…è¯» MiniOneRec çš„ `sft.py`ï¼Œå¯¹ç…§æœ¬èŠ‚å­¦çš„ NTP è®­ç»ƒ\n",
    "- [ ] è¿›å…¥ Step 5ï¼šé˜…è¯» `rl.py`ï¼Œç†è§£ GRPO å¦‚ä½•åœ¨ NTP åŸºç¡€ä¸Šä¼˜åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "> **Step 3 å®Œæˆï¼** ä½ ç°åœ¨ç†è§£äº†ï¼š\n",
    "> 1. Decoder-Only GPT çš„ç»“æ„å’Œ NTP è®­ç»ƒ\n",
    "> 2. LLaMA çš„ä¸‰å¤§æ”¹åŠ¨ï¼šRMSNormã€SwiGLUã€GQA\n",
    "> 3. 4 ç§ç”Ÿæˆç­–ç•¥ + KV Cache åŠ é€Ÿ\n",
    "> 4. MiniOneRec å¦‚ä½•æŠŠæ¨èå˜æˆ token ç”Ÿæˆé—®é¢˜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}