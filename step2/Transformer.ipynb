{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afebd296",
   "metadata": {},
   "source": [
    "## 1.Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a913db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9632,  0.6525,  1.2353,  1.6336],\n",
      "        [-0.8140,  0.3155, -0.3363, -0.8945],\n",
      "        [ 0.4698, -0.8110,  0.4505, -0.2385],\n",
      "        [-0.8686,  1.0179, -0.7343,  1.1894],\n",
      "        [-1.3481, -2.0940,  0.3460,  1.0921],\n",
      "        [ 1.6486, -0.6314,  1.3420,  1.5739],\n",
      "        [ 1.3399, -0.8183,  0.5063,  1.2263],\n",
      "        [-0.2652, -1.8749, -0.5610, -1.6820],\n",
      "        [ 0.2110,  1.3561, -0.4643, -0.4526]], requires_grad=True)\n",
      "tensor([[4, 3, 0, 0, 0],\n",
      "        [7, 4, 1, 1, 0]])\n",
      "tensor([[[-1.3481, -2.0940,  0.3460,  1.0921],\n",
      "         [-0.8686,  1.0179, -0.7343,  1.1894],\n",
      "         [-0.9632,  0.6525,  1.2353,  1.6336],\n",
      "         [-0.9632,  0.6525,  1.2353,  1.6336],\n",
      "         [-0.9632,  0.6525,  1.2353,  1.6336]],\n",
      "\n",
      "        [[-0.2652, -1.8749, -0.5610, -1.6820],\n",
      "         [-1.3481, -2.0940,  0.3460,  1.0921],\n",
      "         [-0.8140,  0.3155, -0.3363, -0.8945],\n",
      "         [-0.8140,  0.3155, -0.3363, -0.8945],\n",
      "         [-0.9632,  0.6525,  1.2353,  1.6336]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding, 以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列, 序列的字符以其在词表中的索引的形式表示\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "model_dim = 4  # embedding维度\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "# src_len = torch.randint(2, 5, (batch_size,))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成源句子和目标句子,构建batch， 并且做了padding, 默认值为0\n",
    "# src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) for L in src_len])\n",
    "\n",
    "sequences = []\n",
    "for L in src_len:\n",
    "    seq = torch.randint(1, max_num_src_words, (L,)) # 随机序列\n",
    "    seq_padded = F.pad(seq, (0, max_src_seq_len - L)) # 右填充\n",
    "    sequences.append(torch.unsqueeze(seq_padded, 0)) # 增加batch维度\n",
    "src_seq = torch.cat(sequences, 0) # 拼接成batch\n",
    "\n",
    "\n",
    "# tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) for L in tgt_len])\n",
    "\n",
    "sequences = []\n",
    "for L in tgt_len:\n",
    "    seq = torch.randint(1, max_num_tgt_words, (L,)) # 随机序列\n",
    "    seq_padded = F.pad(seq, (0, max_tgt_seq_len - L)) # 右填充\n",
    "    sequences.append(torch.unsqueeze(seq_padded, 0)) # 增加batch维度\n",
    "tgt_seq = torch.cat(sequences, 0) # 拼接成batch\n",
    "\n",
    "# 构造embedding\n",
    "src_Embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)  # +1是因为0号索引是padding\n",
    "tgt_Embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)  # +1是因为0号索引是padding\n",
    "src_Embedding = src_Embedding_table(src_seq)\n",
    "tgt_Embedding = tgt_Embedding_table(tgt_seq)\n",
    "\n",
    "print(src_Embedding_table.weight)\n",
    "print(src_seq)\n",
    "print(src_Embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32h1kzmzumr",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Word Embedding 演示了序列建模中如何构建词嵌入：\n",
    "\n",
    "```\n",
    "词索引序列 → Padding 对齐 → Embedding 查表 → 稠密向量\n",
    "```\n",
    "\n",
    "**步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 定义参数 | `batch_size=2, model_dim=4` | 2 个样本，嵌入维度 4 |\n",
    "| 2. 生成序列 | `torch.randint(1, 8, (L,))` | 随机生成词索引 |\n",
    "| 3. Padding | `F.pad(seq, (0, max_len-L))` | 右侧填 0，对齐长度 |\n",
    "| 4. 组 batch | `torch.cat(..., dim=0)` | 拼成 `(batch, seq_len)` |\n",
    "| 5. 嵌入 | `nn.Embedding(9, 4)` | 查表得到向量 |\n",
    "\n",
    "**形状变化：**\n",
    "\n",
    "```\n",
    "src_seq:       (2, 5)      # 2 个句子，每句 5 个词索引\n",
    "src_Embedding: (2, 5, 4)   # 每个词变成 4 维向量\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7p8bmlenq",
   "metadata": {},
   "source": [
    "## 2.Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "uayz17w2y5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码 shape: torch.Size([5, 4])\n",
      "位置编码:\n",
      " tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992]])\n",
      "\n",
      "最终输入 shape: torch.Size([2, 5, 4])\n",
      "最终输入 (Word + Position):\n",
      " tensor([[[-1.3481e+00, -1.0940e+00,  3.4596e-01,  2.0921e+00],\n",
      "         [-2.7103e-02,  1.5582e+00, -7.2429e-01,  2.1893e+00],\n",
      "         [-5.3930e-02,  2.3636e-01,  1.2553e+00,  2.6334e+00],\n",
      "         [-8.2211e-01, -3.3748e-01,  1.2653e+00,  2.6331e+00],\n",
      "         [-1.7200e+00, -1.1328e-03,  1.2753e+00,  2.6328e+00]],\n",
      "\n",
      "        [[-2.6524e-01, -8.7486e-01, -5.6096e-01, -6.8205e-01],\n",
      "         [-5.0660e-01, -1.5537e+00,  3.5596e-01,  2.0921e+00],\n",
      "         [ 9.5303e-02, -1.0065e-01, -3.1635e-01,  1.0531e-01],\n",
      "         [-6.7287e-01, -6.7449e-01, -3.0635e-01,  1.0506e-01],\n",
      "         [-1.7200e+00, -1.1328e-03,  1.2753e+00,  2.6328e+00]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 正弦位置编码\n",
    "# pos: 位置索引 (0, 1, 2, ...)\n",
    "# i: 维度索引\n",
    "# PE(pos, 2i)   = sin(pos / 10000^(2i/d))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n",
    "\n",
    "def sinusoidal_position_embedding(max_len, dim):\n",
    "    \"\"\"\n",
    "    生成正弦位置编码\n",
    "    max_len: 最大序列长度\n",
    "    dim: embedding维度\n",
    "    返回: (max_len, dim) 的位置编码矩阵\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, dim)\n",
    "    pos = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "    # 计算分母: 10000^(2i/d) = exp(2i * log(10000) / d)\n",
    "    div = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "    pe[:, 0::2] = torch.sin(pos * div)  # 偶数维度用sin\n",
    "    pe[:, 1::2] = torch.cos(pos * div)  # 奇数维度用cos\n",
    "    return pe\n",
    "\n",
    "# 生成位置编码\n",
    "src_pos_embedding = sinusoidal_position_embedding(max_src_seq_len, model_dim)  # (5, 4)\n",
    "tgt_pos_embedding = sinusoidal_position_embedding(max_tgt_seq_len, model_dim)  # (5, 4)\n",
    "\n",
    "# Word Embedding + Position Embedding\n",
    "src_input = src_Embedding + src_pos_embedding  # (2, 5, 4) + (5, 4) 广播\n",
    "tgt_input = tgt_Embedding + tgt_pos_embedding  # (2, 5, 4) + (5, 4) 广播\n",
    "\n",
    "print(\"位置编码 shape:\", src_pos_embedding.shape)\n",
    "print(\"位置编码:\\n\", src_pos_embedding)\n",
    "print(\"\\n最终输入 shape:\", src_input.shape)\n",
    "print(\"最终输入 (Word + Position):\\n\", src_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fu84vdjds",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Position Embedding 为序列注入位置信息，与 Word Embedding 相加作为 Transformer 输入：\n",
    "\n",
    "```\n",
    "Word Embedding + Position Embedding = 最终输入\n",
    "   (语义信息)       (位置信息)\n",
    "```\n",
    "\n",
    "**正弦位置编码公式：**\n",
    "- 偶数维度：$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$\n",
    "- 奇数维度：$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 位置索引 | `pos = torch.arange(0, max_len).unsqueeze(1)` | 形状 (seq_len, 1) |\n",
    "| 2. 频率分母 | `div = torch.exp(torch.arange(0, dim, 2) * -(log(10000)/dim))` | 等价于 1/10000^(2i/d) |\n",
    "| 3. 填充 sin/cos | `pe[:, 0::2] = sin`, `pe[:, 1::2] = cos` | 偶数列 sin，奇数列 cos |\n",
    "| 4. 广播相加 | `src_Embedding + src_pos_embedding` | (2,5,4) + (5,4) → (2,5,4) |\n",
    "\n",
    "**为什么用 sin/cos 成对？**\n",
    "- 可通过线性变换表示相对位置：$PE_{pos+k} = M \\cdot PE_{pos}$\n",
    "- 模型能学习\"词与词之间的距离\"\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "src_pos_embedding: (5, 4)      # 位置编码\n",
    "src_Embedding:     (2, 5, 4)   # 词嵌入\n",
    "src_input:         (2, 5, 4)   # 广播相加，最终输入\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9dw2t3o4d",
   "metadata": {},
   "source": [
    "## 3.Encoder Self-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swg4zv1uwke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效位置向量 shape: torch.Size([2, 5])\n",
      "有效位置向量:\n",
      " tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n",
      "\n",
      "Encoder Self-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask:\n",
      " tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "样本1的mask:\n",
      " tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "最终mask (bool):\n",
      " tensor([[[ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "# Encoder Self-Attention Mask\n",
    "# 目的：屏蔽 padding 位置，不让它们参与注意力计算\n",
    "\n",
    "# 第一步：构建有效位置向量 (batch, seq_len)\n",
    "# 1 = 有效位置，0 = padding\n",
    "valid_encoder_pos = torch.cat([\n",
    "    torch.unsqueeze(\n",
    "        torch.cat([torch.ones(L), torch.zeros(max_src_seq_len - L)]), 0\n",
    "    ) \n",
    "    for L in src_len\n",
    "])\n",
    "\n",
    "print(\"有效位置向量 shape:\", valid_encoder_pos.shape)\n",
    "print(\"有效位置向量:\\n\", valid_encoder_pos)\n",
    "# 样本0: [1, 1, 0, 0, 0]  前2个有效\n",
    "# 样本1: [1, 1, 1, 1, 0]  前4个有效\n",
    "\n",
    "# 第二步：扩展成矩阵 (batch, seq_len, seq_len)\n",
    "# 外积: (batch, seq_len, 1) × (batch, 1, seq_len)\n",
    "valid_encoder_pos_matrix = torch.bmm(\n",
    "    valid_encoder_pos.unsqueeze(-1),  # (2, 5, 1)，2个（5，1）的列向量\n",
    "    valid_encoder_pos.unsqueeze(1)    # (2, 1, 5)，2个（1，5）的行向量\n",
    ")\n",
    "\n",
    "print(\"\\nEncoder Self-Attention Mask shape:\", valid_encoder_pos_matrix.shape)\n",
    "print(\"样本0的mask:\\n\", valid_encoder_pos_matrix[0])\n",
    "print(\"样本1的mask:\\n\", valid_encoder_pos_matrix[1])\n",
    "\n",
    "# 转换为 bool 类型，后续用于屏蔽\n",
    "encoder_self_attn_mask = valid_encoder_pos_matrix.bool()\n",
    "print(\"\\n最终mask (bool):\\n\", encoder_self_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qed7qf66tbq",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Encoder Self-Attention Mask 用于屏蔽 padding 位置，防止无意义的填充参与注意力计算：\n",
    "\n",
    "```\n",
    "有效位置 → 正常计算注意力\n",
    "padding  → 屏蔽（softmax 后权重为 0）\n",
    "```\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 有效位置向量 | `torch.cat([ones(L), zeros(max_len-L)])` | 1=有效，0=padding |\n",
    "| 2. 扩展成矩阵 | `torch.bmm(pos.unsqueeze(-1), pos.unsqueeze(1))` | 外积得到 (batch, seq, seq) |\n",
    "| 3. 转布尔类型 | `.bool()` | 方便后续 masked_fill |\n",
    "\n",
    "**外积原理：**\n",
    "```\n",
    "列向量 (5,1) × 行向量 (1,5) → 矩阵 (5,5)\n",
    "\n",
    "[1]                     [[1,1,0,0,0],\n",
    "[1]   × [1,1,0,0,0]  =   [1,1,0,0,0],\n",
    "[0]                      [0,0,0,0,0],\n",
    "[0]                      [0,0,0,0,0],\n",
    "[0]                      [0,0,0,0,0]]\n",
    "```\n",
    "\n",
    "**Mask 含义：**\n",
    "- `mask[i][j] = True`：query_i 可以看 key_j\n",
    "- `mask[i][j] = False`：屏蔽，query_i 不能看 key_j\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "valid_encoder_pos:        (2, 5)      # 有效位置向量\n",
    "valid_encoder_pos_matrix: (2, 5, 5)   # 注意力 mask 矩阵\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ipu2twurj2",
   "metadata": {},
   "source": [
    "## 4.Intra-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9h716ybp8hc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder 有效位置: tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n",
      "Decoder 有效位置: tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "\n",
      "Cross-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask (tgt_len=4, src_len=2):\n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "样本1的mask (tgt_len=3, src_len=4):\n",
      " tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Intra-Attention Mask (Cross-Attention)\n",
    "# 目的：Decoder 看 Encoder 时，屏蔽双方的 padding\n",
    "\n",
    "# Decoder 有效位置 (batch, tgt_seq_len)\n",
    "valid_decoder_pos = torch.cat([\n",
    "    torch.unsqueeze(\n",
    "        torch.cat([torch.ones(L), torch.zeros(max_tgt_seq_len - L)]), 0\n",
    "    ) \n",
    "    for L in tgt_len\n",
    "])\n",
    "\n",
    "print(\"Encoder 有效位置:\", valid_encoder_pos)  # src_len = [2, 4]\n",
    "print(\"Decoder 有效位置:\", valid_decoder_pos)  # tgt_len = [4, 3]\n",
    "\n",
    "# Cross-Attention Mask: (batch, tgt_seq_len, src_seq_len)\n",
    "# Decoder query (tgt) × Encoder key (src)\n",
    "cross_attn_mask = torch.bmm(\n",
    "    valid_decoder_pos.unsqueeze(-1),  # (2, 5, 1)\n",
    "    valid_encoder_pos.unsqueeze(1)    # (2, 1, 5)\n",
    ").bool()\n",
    "\n",
    "print(\"\\nCross-Attention Mask shape:\", cross_attn_mask.shape)\n",
    "print(\"样本0的mask (tgt_len=4, src_len=2):\\n\", cross_attn_mask[0].int())\n",
    "print(\"样本1的mask (tgt_len=3, src_len=4):\\n\", cross_attn_mask[1].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgrb3fzmgh",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Cross-Attention 让 Decoder 查询 Encoder 的信息，Mask 屏蔽双方的 padding：\n",
    "\n",
    "```\n",
    "Decoder (Q) → 发起查询：\"源句子哪部分和我相关？\"\n",
    "Encoder (KV) → 被查询：提供源句子的语义信息\n",
    "```\n",
    "\n",
    "**与 Self-Attention 的区别：**\n",
    "\n",
    "| 类型 | Query | Key/Value | Mask 形状 |\n",
    "|------|-------|-----------|-----------|\n",
    "| Self-Attention | 自己 | 自己 | (batch, seq, seq) |\n",
    "| Cross-Attention | Decoder | Encoder | (batch, tgt_len, src_len) |\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. Decoder 有效位置 | `valid_decoder_pos` | tgt_len = [4, 3] |\n",
    "| 2. Encoder 有效位置 | `valid_encoder_pos` | src_len = [2, 4] |\n",
    "| 3. 外积得 Mask | `bmm(decoder.unsqueeze(-1), encoder.unsqueeze(1))` | (batch, tgt_len, src_len) |\n",
    "\n",
    "**Mask 含义：**\n",
    "- `mask[i][j] = True`：Decoder 位置 i 可以看 Encoder 位置 j\n",
    "- `mask[i][j] = False`：屏蔽（任一方是 padding）\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "valid_decoder_pos: (2, 5)       # Decoder 有效位置\n",
    "valid_encoder_pos: (2, 5)       # Encoder 有效位置\n",
    "cross_attn_mask:   (2, 5, 5)    # Cross-Attention Mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09gkb9jjbwp",
   "metadata": {},
   "source": [
    "## 5.Decoder Self-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oifs2drppb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask (下三角):\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Padding Mask 样本0 (tgt_len=4):\n",
      "tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Decoder Self-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask (tgt_len=4):\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "样本1的mask (tgt_len=3):\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Decoder Self-Attention Mask\n",
    "# 目的：1. 屏蔽 padding  2. 屏蔽未来位置（不能偷看答案）\n",
    "\n",
    "# 第一步：Causal Mask（因果掩码）- 下三角矩阵\n",
    "# 每个位置只能看自己和之前的位置\n",
    "causal_mask = torch.tril(torch.ones(max_tgt_seq_len, max_tgt_seq_len))\n",
    "print(\"Causal Mask (下三角):\")\n",
    "print(causal_mask)\n",
    "\n",
    "# 第二步：Padding Mask - 有效位置矩阵\n",
    "valid_decoder_pos_matrix = torch.bmm(\n",
    "    valid_decoder_pos.unsqueeze(-1),  # (2, 5, 1)\n",
    "    valid_decoder_pos.unsqueeze(1)    # (2, 1, 5)\n",
    ")\n",
    "print(\"\\nPadding Mask 样本0 (tgt_len=4):\")\n",
    "print(valid_decoder_pos_matrix[0])\n",
    "\n",
    "# 第三步：两者相乘（同时满足两个条件）\n",
    "decoder_self_attn_mask = (causal_mask * valid_decoder_pos_matrix).bool() # 哈达玛积，对应元素相乘\n",
    "\n",
    "print(\"\\nDecoder Self-Attention Mask shape:\", decoder_self_attn_mask.shape)\n",
    "print(\"样本0的mask (tgt_len=4):\\n\", decoder_self_attn_mask[0].int())\n",
    "print(\"样本1的mask (tgt_len=3):\\n\", decoder_self_attn_mask[1].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vjv48f7jzj",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Decoder Self-Attention Mask 需要同时满足两个约束：\n",
    "\n",
    "```\n",
    "1. Causal Mask：不能看未来（防止作弊）\n",
    "2. Padding Mask：不能看 padding（无意义填充）\n",
    "```\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. Causal Mask | `torch.tril(torch.ones(seq_len, seq_len))` | 下三角矩阵 |\n",
    "| 2. Padding Mask | `bmm(pos.unsqueeze(-1), pos.unsqueeze(1))` | 有效位置矩阵 |\n",
    "| 3. 相乘 | `causal_mask * padding_mask` | Hadamard Product，同时满足两个条件 |\n",
    "\n",
    "**三种 Mask 对比：**\n",
    "\n",
    "| Mask | 约束 | 形状 |\n",
    "|------|------|------|\n",
    "| Encoder Self-Attention | 只屏蔽 padding | (batch, src_len, src_len) |\n",
    "| Cross-Attention | 屏蔽双方 padding | (batch, tgt_len, src_len) |\n",
    "| Decoder Self-Attention | padding + 未来 | (batch, tgt_len, tgt_len) |\n",
    "\n",
    "**Causal Mask 示意：**\n",
    "```\n",
    "位置0: [1, 0, 0, 0, 0]  ← 只能看自己\n",
    "位置1: [1, 1, 0, 0, 0]  ← 能看 0, 1\n",
    "位置2: [1, 1, 1, 0, 0]  ← 能看 0, 1, 2\n",
    "...\n",
    "```\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "causal_mask:           (5, 5)      # 下三角\n",
    "valid_decoder_pos_matrix: (2, 5, 5)   # padding mask\n",
    "decoder_self_attn_mask:   (2, 5, 5)   # 最终 mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1rq4i2falc5",
   "metadata": {},
   "source": [
    "## 6.Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "g4tua3mwp25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dim=4, num_heads=2, head_dim=2\n",
      "Q shape: torch.Size([2, 5, 4])\n",
      "Q after reshape: torch.Size([2, 2, 5, 2])\n",
      "Attention scores shape: torch.Size([2, 2, 5, 5])\n",
      "tensor([[-0.2136,  0.1260, -0.1885, -0.2376, -0.1703],\n",
      "        [-0.1974, -0.2793, -0.3767, -0.3894, -0.5325],\n",
      "        [-0.7051, -0.0283, -0.8493, -0.9746, -0.9829],\n",
      "        [-0.6870,  0.1070, -0.7586, -0.8918, -0.8301],\n",
      "        [-0.8434,  0.2553, -0.8680, -1.0418, -0.9018]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Scores after mask:\n",
      " tensor([[-2.1362e-01,  1.2597e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.9742e-01, -2.7935e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Attention weights shape: torch.Size([2, 2, 5, 5])\n",
      "Attention weights (样本0, 头0):\n",
      " tensor([[0.4159, 0.5841, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5205, 0.4795, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]], grad_fn=<SelectBackward0>)\n",
      "Attention output shape: torch.Size([2, 2, 5, 2])\n",
      "After concat shape: torch.Size([2, 5, 4])\n",
      "Final output shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head Self-Attention\n",
    "# 核心公式: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "\n",
    "import math\n",
    "\n",
    "# 参数设置\n",
    "num_heads = 2\n",
    "head_dim = model_dim // num_heads  # 4 // 2 = 2\n",
    "\n",
    "print(f\"model_dim={model_dim}, num_heads={num_heads}, head_dim={head_dim}\")\n",
    "\n",
    "# 四个线性层: W_Q, W_K, W_V, W_O\n",
    "W_Q = nn.Linear(model_dim, model_dim)\n",
    "W_K = nn.Linear(model_dim, model_dim)\n",
    "W_V = nn.Linear(model_dim, model_dim)\n",
    "W_O = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "# 第一步: 计算 Q, K, V\n",
    "Q = W_Q(src_input)  # (batch, seq_len, model_dim) = (2, 5, 4)\n",
    "K = W_K(src_input)\n",
    "V = W_V(src_input)\n",
    "print(\"Q shape:\", Q.shape)\n",
    "\n",
    "# 第二步: 拆成多头\n",
    "# (batch, seq_len, model_dim) → (batch, seq_len, num_heads, head_dim) → (batch, num_heads, seq_len, head_dim)\n",
    "Q = Q.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)  # -1自动计算seq_len\n",
    "K = K.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n",
    "print(\"Q after reshape:\", Q.shape)  # (2, 2, 5, 2)\n",
    "\n",
    "# 第三步: 计算注意力分数 QK^T / sqrt(d_k)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "print(\"Attention scores shape:\", scores.shape)  # (2, 2, 5, 5)\n",
    "print(scores[0, 0])  # 样本0，头0\n",
    "\n",
    "# 第四步: 应用 Mask (扩展 mask 维度以匹配多头)\n",
    "# encoder_self_attn_mask: (batch, seq_len, seq_len) → (batch, 1, seq_len, seq_len)\n",
    "mask_expanded = encoder_self_attn_mask.unsqueeze(1)  # (2, 1, 5, 5)\n",
    "scores = scores.masked_fill(~mask_expanded, -1e9)\n",
    "print(\"Scores after mask:\\n\", scores[0, 0])  # 样本0，头0\n",
    "\n",
    "# 第五步: Softmax 得到注意力权重\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "print(\"Attention weights (样本0, 头0):\\n\", attn_weights[0, 0])\n",
    "\n",
    "# 第六步: 加权求和 weights * V\n",
    "attn_output = torch.matmul(attn_weights, V)\n",
    "print(\"Attention output shape:\", attn_output.shape)  # (2, 2, 5, 2)\n",
    "\n",
    "# 第七步: 拼接多头\n",
    "# (batch, num_heads, seq_len, head_dim) → (batch, seq_len, num_heads, head_dim) → (batch, seq_len, model_dim)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, model_dim)\n",
    "print(\"After concat shape:\", attn_output.shape)  # (2, 5, 4)\n",
    "\n",
    "# 第八步: 输出投影\n",
    "output = W_O(attn_output)\n",
    "print(\"Final output shape:\", output.shape)  # (2, 5, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
