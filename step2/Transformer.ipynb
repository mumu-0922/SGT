{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afebd296",
   "metadata": {},
   "source": [
    "## 1.Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a913db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4838, -1.6936,  0.3788, -1.0152],\n",
      "        [-0.3222, -0.5543,  1.1450, -0.3491],\n",
      "        [-0.5018,  0.6509, -2.5256, -0.0114],\n",
      "        [ 0.5796,  1.5169,  0.5152, -1.1080],\n",
      "        [ 1.0572, -1.0375, -1.1819, -0.9473],\n",
      "        [ 1.3396,  0.3323, -0.0717,  2.4519],\n",
      "        [ 0.2083,  1.2658, -1.2404,  1.6540],\n",
      "        [-0.7176,  0.0196,  1.0457, -1.0488],\n",
      "        [-0.2819,  0.3302,  0.7387, -1.6437]], requires_grad=True)\n",
      "tensor([[7, 7, 0, 0, 0],\n",
      "        [4, 6, 7, 5, 0]])\n",
      "tensor([[[-0.7176,  0.0196,  1.0457, -1.0488],\n",
      "         [-0.7176,  0.0196,  1.0457, -1.0488],\n",
      "         [ 0.4838, -1.6936,  0.3788, -1.0152],\n",
      "         [ 0.4838, -1.6936,  0.3788, -1.0152],\n",
      "         [ 0.4838, -1.6936,  0.3788, -1.0152]],\n",
      "\n",
      "        [[ 1.0572, -1.0375, -1.1819, -0.9473],\n",
      "         [ 0.2083,  1.2658, -1.2404,  1.6540],\n",
      "         [-0.7176,  0.0196,  1.0457, -1.0488],\n",
      "         [ 1.3396,  0.3323, -0.0717,  2.4519],\n",
      "         [ 0.4838, -1.6936,  0.3788, -1.0152]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding, 以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列, 序列的字符以其在词表中的索引的形式表示\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "model_dim = 4  # embedding维度\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "# src_len = torch.randint(2, 5, (batch_size,))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成源句子和目标句子,构建batch， 并且做了padding, 默认值为0\n",
    "# src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) for L in src_len])\n",
    "\n",
    "sequences = []\n",
    "for L in src_len:\n",
    "    seq = torch.randint(1, max_num_src_words, (L,)) # 随机序列\n",
    "    seq_padded = F.pad(seq, (0, max_src_seq_len - L)) # 右填充\n",
    "    sequences.append(torch.unsqueeze(seq_padded, 0)) # 增加batch维度\n",
    "src_seq = torch.cat(sequences, 0) # 拼接成batch\n",
    "\n",
    "\n",
    "# tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) for L in tgt_len])\n",
    "\n",
    "sequences = []\n",
    "for L in tgt_len:\n",
    "    seq = torch.randint(1, max_num_tgt_words, (L,)) # 随机序列\n",
    "    seq_padded = F.pad(seq, (0, max_tgt_seq_len - L)) # 右填充\n",
    "    sequences.append(torch.unsqueeze(seq_padded, 0)) # 增加batch维度\n",
    "tgt_seq = torch.cat(sequences, 0) # 拼接成batch\n",
    "\n",
    "# 构造embedding\n",
    "src_Embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)  # +1是因为0号索引是padding\n",
    "tgt_Embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)  # +1是因为0号索引是padding\n",
    "src_Embedding = src_Embedding_table(src_seq)\n",
    "tgt_Embedding = tgt_Embedding_table(tgt_seq)\n",
    "\n",
    "print(src_Embedding_table.weight)\n",
    "print(src_seq)\n",
    "print(src_Embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32h1kzmzumr",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Word Embedding 演示了序列建模中如何构建词嵌入：\n",
    "\n",
    "```\n",
    "词索引序列 → Padding 对齐 → Embedding 查表 → 稠密向量\n",
    "```\n",
    "\n",
    "**步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 定义参数 | `batch_size=2, model_dim=4` | 2 个样本，嵌入维度 4 |\n",
    "| 2. 生成序列 | `torch.randint(1, 8, (L,))` | 随机生成词索引 |\n",
    "| 3. Padding | `F.pad(seq, (0, max_len-L))` | 右侧填 0，对齐长度 |\n",
    "| 4. 组 batch | `torch.cat(..., dim=0)` | 拼成 `(batch, seq_len)` |\n",
    "| 5. 嵌入 | `nn.Embedding(9, 4)` | 查表得到向量 |\n",
    "\n",
    "**形状变化：**\n",
    "\n",
    "```\n",
    "src_seq:       (2, 5)      # 2 个句子，每句 5 个词索引\n",
    "src_Embedding: (2, 5, 4)   # 每个词变成 4 维向量\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7p8bmlenq",
   "metadata": {},
   "source": [
    "## 2.Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uayz17w2y5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码 shape: torch.Size([5, 4])\n",
      "位置编码:\n",
      " tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992]])\n",
      "\n",
      "最终输入 shape: torch.Size([2, 5, 4])\n",
      "最终输入 (Word + Position):\n",
      " tensor([[[-0.7176,  1.0196,  1.0457, -0.0488],\n",
      "         [ 0.1238,  0.5599,  1.0557, -0.0489],\n",
      "         [ 1.3931, -2.1098,  0.3988, -0.0154],\n",
      "         [ 0.6249, -2.6836,  0.4088, -0.0156],\n",
      "         [-0.2730, -2.3473,  0.4188, -0.0160]],\n",
      "\n",
      "        [[ 1.0572, -0.0375, -1.1819,  0.0527],\n",
      "         [ 1.0498,  1.8061, -1.2304,  2.6540],\n",
      "         [ 0.1917, -0.3965,  1.0657, -0.0490],\n",
      "         [ 1.4807, -0.6577, -0.0417,  3.4514],\n",
      "         [-0.2730, -2.3473,  0.4188, -0.0160]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 正弦位置编码\n",
    "# pos: 位置索引 (0, 1, 2, ...)\n",
    "# i: 维度索引\n",
    "# PE(pos, 2i)   = sin(pos / 10000^(2i/d))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n",
    "\n",
    "def sinusoidal_position_embedding(max_len, dim):\n",
    "    \"\"\"\n",
    "    生成正弦位置编码\n",
    "    max_len: 最大序列长度\n",
    "    dim: embedding维度\n",
    "    返回: (max_len, dim) 的位置编码矩阵\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, dim)\n",
    "    pos = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "    # 计算分母: 10000^(2i/d) = exp(2i * log(10000) / d)\n",
    "    div = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "    pe[:, 0::2] = torch.sin(pos * div)  # 偶数维度用sin\n",
    "    pe[:, 1::2] = torch.cos(pos * div)  # 奇数维度用cos\n",
    "    return pe\n",
    "\n",
    "# 生成位置编码\n",
    "src_pos_embedding = sinusoidal_position_embedding(max_src_seq_len, model_dim)  # (5, 4)\n",
    "tgt_pos_embedding = sinusoidal_position_embedding(max_tgt_seq_len, model_dim)  # (5, 4)\n",
    "\n",
    "# Word Embedding + Position Embedding\n",
    "src_input = src_Embedding + src_pos_embedding  # (2, 5, 4) + (5, 4) 广播\n",
    "tgt_input = tgt_Embedding + tgt_pos_embedding  # (2, 5, 4) + (5, 4) 广播\n",
    "\n",
    "print(\"位置编码 shape:\", src_pos_embedding.shape)\n",
    "print(\"位置编码:\\n\", src_pos_embedding)\n",
    "print(\"\\n最终输入 shape:\", src_input.shape)\n",
    "print(\"最终输入 (Word + Position):\\n\", src_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fu84vdjds",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Position Embedding 为序列注入位置信息，与 Word Embedding 相加作为 Transformer 输入：\n",
    "\n",
    "```\n",
    "Word Embedding + Position Embedding = 最终输入\n",
    "   (语义信息)       (位置信息)\n",
    "```\n",
    "\n",
    "**正弦位置编码公式：**\n",
    "- 偶数维度：$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$\n",
    "- 奇数维度：$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 位置索引 | `pos = torch.arange(0, max_len).unsqueeze(1)` | 形状 (seq_len, 1) |\n",
    "| 2. 频率分母 | `div = torch.exp(torch.arange(0, dim, 2) * -(log(10000)/dim))` | 等价于 1/10000^(2i/d) |\n",
    "| 3. 填充 sin/cos | `pe[:, 0::2] = sin`, `pe[:, 1::2] = cos` | 偶数列 sin，奇数列 cos |\n",
    "| 4. 广播相加 | `src_Embedding + src_pos_embedding` | (2,5,4) + (5,4) → (2,5,4) |\n",
    "\n",
    "**为什么用 sin/cos 成对？**\n",
    "- 可通过线性变换表示相对位置：$PE_{pos+k} = M \\cdot PE_{pos}$\n",
    "- 模型能学习\"词与词之间的距离\"\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "src_pos_embedding: (5, 4)      # 位置编码\n",
    "src_Embedding:     (2, 5, 4)   # 词嵌入\n",
    "src_input:         (2, 5, 4)   # 广播相加，最终输入\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9dw2t3o4d",
   "metadata": {},
   "source": [
    "## 3.Encoder Self-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swg4zv1uwke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效位置向量 shape: torch.Size([2, 5])\n",
      "有效位置向量:\n",
      " tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n",
      "\n",
      "Encoder Self-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask:\n",
      " tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "样本1的mask:\n",
      " tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "最终mask (bool):\n",
      " tensor([[[ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "# Encoder Self-Attention Mask\n",
    "# 目的：屏蔽 padding 位置，不让它们参与注意力计算\n",
    "\n",
    "# 第一步：构建有效位置向量 (batch, seq_len)\n",
    "# 1 = 有效位置，0 = padding\n",
    "valid_encoder_pos = torch.cat([\n",
    "    torch.unsqueeze(\n",
    "        torch.cat([torch.ones(L), torch.zeros(max_src_seq_len - L)]), 0\n",
    "    ) \n",
    "    for L in src_len\n",
    "])\n",
    "\n",
    "print(\"有效位置向量 shape:\", valid_encoder_pos.shape)\n",
    "print(\"有效位置向量:\\n\", valid_encoder_pos)\n",
    "# 样本0: [1, 1, 0, 0, 0]  前2个有效\n",
    "# 样本1: [1, 1, 1, 1, 0]  前4个有效\n",
    "\n",
    "# 第二步：扩展成矩阵 (batch, seq_len, seq_len)\n",
    "# 外积: (batch, seq_len, 1) × (batch, 1, seq_len)\n",
    "valid_encoder_pos_matrix = torch.bmm(\n",
    "    valid_encoder_pos.unsqueeze(-1),  # (2, 5, 1)，2个（5，1）的列向量\n",
    "    valid_encoder_pos.unsqueeze(1)    # (2, 1, 5)，2个（1，5）的行向量\n",
    ")\n",
    "\n",
    "print(\"\\nEncoder Self-Attention Mask shape:\", valid_encoder_pos_matrix.shape)\n",
    "print(\"样本0的mask:\\n\", valid_encoder_pos_matrix[0])\n",
    "print(\"样本1的mask:\\n\", valid_encoder_pos_matrix[1])\n",
    "\n",
    "# 转换为 bool 类型，后续用于屏蔽\n",
    "encoder_self_attn_mask = valid_encoder_pos_matrix.bool()\n",
    "print(\"\\n最终mask (bool):\\n\", encoder_self_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qed7qf66tbq",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Encoder Self-Attention Mask 用于屏蔽 padding 位置，防止无意义的填充参与注意力计算：\n",
    "\n",
    "```\n",
    "有效位置 → 正常计算注意力\n",
    "padding  → 屏蔽（softmax 后权重为 0）\n",
    "```\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 有效位置向量 | `torch.cat([ones(L), zeros(max_len-L)])` | 1=有效，0=padding |\n",
    "| 2. 扩展成矩阵 | `torch.bmm(pos.unsqueeze(-1), pos.unsqueeze(1))` | 外积得到 (batch, seq, seq) |\n",
    "| 3. 转布尔类型 | `.bool()` | 方便后续 masked_fill |\n",
    "\n",
    "**外积原理：**\n",
    "```\n",
    "列向量 (5,1) × 行向量 (1,5) → 矩阵 (5,5)\n",
    "\n",
    "[1]                     [[1,1,0,0,0],\n",
    "[1]   × [1,1,0,0,0]  =   [1,1,0,0,0],\n",
    "[0]                      [0,0,0,0,0],\n",
    "[0]                      [0,0,0,0,0],\n",
    "[0]                      [0,0,0,0,0]]\n",
    "```\n",
    "\n",
    "**Mask 含义：**\n",
    "- `mask[i][j] = True`：query_i 可以看 key_j\n",
    "- `mask[i][j] = False`：屏蔽，query_i 不能看 key_j\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "valid_encoder_pos:        (2, 5)      # 有效位置向量\n",
    "valid_encoder_pos_matrix: (2, 5, 5)   # 注意力 mask 矩阵\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ipu2twurj2",
   "metadata": {},
   "source": [
    "## 4.Intra-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9h716ybp8hc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder 有效位置: tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n",
      "Decoder 有效位置: tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "\n",
      "Cross-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask (tgt_len=4, src_len=2):\n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "样本1的mask (tgt_len=3, src_len=4):\n",
      " tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Intra-Attention Mask (Cross-Attention)\n",
    "# 目的：Decoder 看 Encoder 时，屏蔽双方的 padding\n",
    "\n",
    "# Decoder 有效位置 (batch, tgt_seq_len)\n",
    "valid_decoder_pos = torch.cat([\n",
    "    torch.unsqueeze(\n",
    "        torch.cat([torch.ones(L), torch.zeros(max_tgt_seq_len - L)]), 0\n",
    "    ) \n",
    "    for L in tgt_len\n",
    "])\n",
    "\n",
    "print(\"Encoder 有效位置:\", valid_encoder_pos)  # src_len = [2, 4]\n",
    "print(\"Decoder 有效位置:\", valid_decoder_pos)  # tgt_len = [4, 3]\n",
    "\n",
    "# Cross-Attention Mask: (batch, tgt_seq_len, src_seq_len)\n",
    "# Decoder query (tgt) × Encoder key (src)\n",
    "cross_attn_mask = torch.bmm(\n",
    "    valid_decoder_pos.unsqueeze(-1),  # (2, 5, 1)\n",
    "    valid_encoder_pos.unsqueeze(1)    # (2, 1, 5)\n",
    ").bool()\n",
    "\n",
    "print(\"\\nCross-Attention Mask shape:\", cross_attn_mask.shape)\n",
    "print(\"样本0的mask (tgt_len=4, src_len=2):\\n\", cross_attn_mask[0].int())\n",
    "print(\"样本1的mask (tgt_len=3, src_len=4):\\n\", cross_attn_mask[1].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgrb3fzmgh",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Cross-Attention 让 Decoder 查询 Encoder 的信息，Mask 屏蔽双方的 padding：\n",
    "\n",
    "```\n",
    "Decoder (Q) → 发起查询：\"源句子哪部分和我相关？\"\n",
    "Encoder (KV) → 被查询：提供源句子的语义信息\n",
    "```\n",
    "\n",
    "**与 Self-Attention 的区别：**\n",
    "\n",
    "| 类型 | Query | Key/Value | Mask 形状 |\n",
    "|------|-------|-----------|-----------|\n",
    "| Self-Attention | 自己 | 自己 | (batch, seq, seq) |\n",
    "| Cross-Attention | Decoder | Encoder | (batch, tgt_len, src_len) |\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. Decoder 有效位置 | `valid_decoder_pos` | tgt_len = [4, 3] |\n",
    "| 2. Encoder 有效位置 | `valid_encoder_pos` | src_len = [2, 4] |\n",
    "| 3. 外积得 Mask | `bmm(decoder.unsqueeze(-1), encoder.unsqueeze(1))` | (batch, tgt_len, src_len) |\n",
    "\n",
    "**Mask 含义：**\n",
    "- `mask[i][j] = True`：Decoder 位置 i 可以看 Encoder 位置 j\n",
    "- `mask[i][j] = False`：屏蔽（任一方是 padding）\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "valid_decoder_pos: (2, 5)       # Decoder 有效位置\n",
    "valid_encoder_pos: (2, 5)       # Encoder 有效位置\n",
    "cross_attn_mask:   (2, 5, 5)    # Cross-Attention Mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09gkb9jjbwp",
   "metadata": {},
   "source": [
    "## 5.Decoder Self-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oifs2drppb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask (下三角):\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Padding Mask 样本0 (tgt_len=4):\n",
      "tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Decoder Self-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "样本0的mask (tgt_len=4):\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "样本1的mask (tgt_len=3):\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Decoder Self-Attention Mask\n",
    "# 目的：1. 屏蔽 padding  2. 屏蔽未来位置（不能偷看答案）\n",
    "\n",
    "# 第一步：Causal Mask（因果掩码）- 下三角矩阵\n",
    "# 每个位置只能看自己和之前的位置\n",
    "causal_mask = torch.tril(torch.ones(max_tgt_seq_len, max_tgt_seq_len))\n",
    "print(\"Causal Mask (下三角):\")\n",
    "print(causal_mask)\n",
    "\n",
    "# 第二步：Padding Mask - 有效位置矩阵\n",
    "valid_decoder_pos_matrix = torch.bmm(\n",
    "    valid_decoder_pos.unsqueeze(-1),  # (2, 5, 1)\n",
    "    valid_decoder_pos.unsqueeze(1)    # (2, 1, 5)\n",
    ")\n",
    "print(\"\\nPadding Mask 样本0 (tgt_len=4):\")\n",
    "print(valid_decoder_pos_matrix[0])\n",
    "\n",
    "# 第三步：两者相乘（同时满足两个条件）\n",
    "decoder_self_attn_mask = (causal_mask * valid_decoder_pos_matrix).bool() # 哈达玛积，对应元素相乘\n",
    "\n",
    "print(\"\\nDecoder Self-Attention Mask shape:\", decoder_self_attn_mask.shape)\n",
    "print(\"样本0的mask (tgt_len=4):\\n\", decoder_self_attn_mask[0].int())\n",
    "print(\"样本1的mask (tgt_len=3):\\n\", decoder_self_attn_mask[1].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vjv48f7jzj",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Decoder Self-Attention Mask 需要同时满足两个约束：\n",
    "\n",
    "```\n",
    "1. Causal Mask：不能看未来（防止作弊）\n",
    "2. Padding Mask：不能看 padding（无意义填充）\n",
    "```\n",
    "\n",
    "**代码步骤：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. Causal Mask | `torch.tril(torch.ones(seq_len, seq_len))` | 下三角矩阵 |\n",
    "| 2. Padding Mask | `bmm(pos.unsqueeze(-1), pos.unsqueeze(1))` | 有效位置矩阵 |\n",
    "| 3. 相乘 | `causal_mask * padding_mask` | Hadamard Product，同时满足两个条件 |\n",
    "\n",
    "**三种 Mask 对比：**\n",
    "\n",
    "| Mask | 约束 | 形状 |\n",
    "|------|------|------|\n",
    "| Encoder Self-Attention | 只屏蔽 padding | (batch, src_len, src_len) |\n",
    "| Cross-Attention | 屏蔽双方 padding | (batch, tgt_len, src_len) |\n",
    "| Decoder Self-Attention | padding + 未来 | (batch, tgt_len, tgt_len) |\n",
    "\n",
    "**Causal Mask 示意：**\n",
    "```\n",
    "位置0: [1, 0, 0, 0, 0]  ← 只能看自己\n",
    "位置1: [1, 1, 0, 0, 0]  ← 能看 0, 1\n",
    "位置2: [1, 1, 1, 0, 0]  ← 能看 0, 1, 2\n",
    "...\n",
    "```\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "causal_mask:           (5, 5)      # 下三角\n",
    "valid_decoder_pos_matrix: (2, 5, 5)   # padding mask\n",
    "decoder_self_attn_mask:   (2, 5, 5)   # 最终 mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1rq4i2falc5",
   "metadata": {},
   "source": [
    "## 6.Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "g4tua3mwp25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dim=4, num_heads=2, head_dim=2\n",
      "Q shape: torch.Size([2, 5, 4])\n",
      "Q after reshape: torch.Size([2, 2, 5, 2])\n",
      "Attention scores shape: torch.Size([2, 2, 5, 5])\n",
      "tensor([[ 0.1167,  0.0109, -0.2592, -0.1725, -0.0590],\n",
      "        [ 0.1988,  0.0508, -0.4102, -0.3218, -0.1677],\n",
      "        [ 0.3401,  0.2901, -0.5033, -0.7267, -0.7105],\n",
      "        [ 0.3090,  0.3331, -0.3895, -0.7206, -0.7906],\n",
      "        [ 0.2300,  0.3023, -0.2367, -0.5835, -0.7019]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Scores after mask:\n",
      " tensor([[ 1.1668e-01,  1.0923e-02, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 1.9879e-01,  5.0813e-02, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Attention weights shape: torch.Size([2, 2, 5, 5])\n",
      "Attention weights (样本0, 头0):\n",
      " tensor([[0.5264, 0.4736, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5369, 0.4631, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]], grad_fn=<SelectBackward0>)\n",
      "Attention output shape: torch.Size([2, 2, 5, 2])\n",
      "Attention output (样本0, 头0):\n",
      " tensor([[-0.1758, -1.0764],\n",
      "        [-0.1748, -1.0791],\n",
      "        [ 0.1070, -0.3642],\n",
      "        [ 0.1070, -0.3642],\n",
      "        [ 0.1070, -0.3642]], grad_fn=<SelectBackward0>)\n",
      "After concat shape: torch.Size([2, 5, 4])\n",
      "Final output shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head Self-Attention\n",
    "# 核心公式: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "\n",
    "import math\n",
    "\n",
    "# 参数设置\n",
    "num_heads = 2\n",
    "head_dim = model_dim // num_heads  # 4 // 2 = 2\n",
    "\n",
    "print(f\"model_dim={model_dim}, num_heads={num_heads}, head_dim={head_dim}\")\n",
    "\n",
    "# 四个线性层: W_Q, W_K, W_V, W_O\n",
    "W_Q = nn.Linear(model_dim, model_dim)\n",
    "W_K = nn.Linear(model_dim, model_dim)\n",
    "W_V = nn.Linear(model_dim, model_dim)\n",
    "W_O = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "# 第一步: 计算 Q, K, V\n",
    "Q = W_Q(src_input)  # (batch, seq_len, model_dim) = (2, 5, 4)\n",
    "K = W_K(src_input)\n",
    "V = W_V(src_input)\n",
    "print(\"Q shape:\", Q.shape)\n",
    "\n",
    "# 第二步: 拆成多头\n",
    "# (batch, seq_len, model_dim) → (batch, seq_len, num_heads, head_dim) → (batch, num_heads, seq_len, head_dim)\n",
    "Q = Q.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)  # -1自动计算seq_len\n",
    "K = K.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n",
    "print(\"Q after reshape:\", Q.shape)  # (2, 2, 5, 2)\n",
    "\n",
    "# 第三步: 计算注意力分数 QK^T / sqrt(d_k)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "print(\"Attention scores shape:\", scores.shape)  # (2, 2, 5, 5)\n",
    "print(scores[0, 0])  # 样本0，头0\n",
    "\n",
    "# 第四步: 应用 Mask (扩展 mask 维度以匹配多头)\n",
    "# encoder_self_attn_mask: (batch, seq_len, seq_len) → (batch, 1, seq_len, seq_len)\n",
    "mask_expanded = encoder_self_attn_mask.unsqueeze(1)  # (2, 1, 5, 5)\n",
    "scores = scores.masked_fill(~mask_expanded, -1e9)   #  scores.masked_fill(条件, 填充值)\n",
    "  # 条件为 True 的位置 → 填充\n",
    "  # 条件为 False 的位置 → 保持原值\n",
    "print(\"Scores after mask:\\n\", scores[0, 0])  # 样本0，头0\n",
    "\n",
    "# 第五步: Softmax 得到注意力权重\n",
    "attn_weights = F.softmax(scores, dim=-1) # dim=-1 是因为每个 query 对所有 key 的注意力权重要归一化到 1。\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "print(\"Attention weights (样本0, 头0):\\n\", attn_weights[0, 0])\n",
    "\n",
    "# 第六步: 加权求和 weights * V\n",
    "attn_output = torch.matmul(attn_weights, V)\n",
    "print(\"Attention output shape:\", attn_output.shape)  # (2, 2, 5, 2)\n",
    "print(\"Attention output (样本0, 头0):\\n\", attn_output[0, 0])\n",
    "\n",
    "# 第七步: 拼接多头\n",
    "# (batch, num_heads, seq_len, head_dim) → (batch, seq_len, num_heads, head_dim) → (batch, seq_len, model_dim)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, model_dim)\n",
    "print(\"After concat shape:\", attn_output.shape)  # (2, 5, 4)\n",
    "\n",
    "# 第八步: 输出投影\n",
    "output = W_O(attn_output)\n",
    "print(\"Final output shape:\", output.shape)  # (2, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m86ogimnpc",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Multi-Head Self-Attention 是 Transformer 的核心，让每个位置能关注序列中的其他位置：\n",
    "\n",
    "**核心公式：**\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "$$\\text{MultiHead} = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "**完整流程：**\n",
    "\n",
    "```\n",
    "src_input (2, 5, 4)\n",
    "     │\n",
    "     ├──→ W_Q ──→ Q (2, 5, 4)\n",
    "     ├──→ W_K ──→ K (2, 5, 4)\n",
    "     └──→ W_V ──→ V (2, 5, 4)\n",
    "                  │\n",
    "                  ↓ 拆成多头\n",
    "           Q, K, V (2, 2, 5, 2)\n",
    "                  │\n",
    "                  ↓ Q × K^T / √d_k + mask + softmax\n",
    "        attn_weights (2, 2, 5, 5)\n",
    "                  │\n",
    "                  ↓ × V (加权求和)\n",
    "        attn_output (2, 2, 5, 2)\n",
    "                  │\n",
    "                  ↓ 拼接多头 + W_O\n",
    "            output (2, 5, 4)\n",
    "```\n",
    "\n",
    "**八步代码：**\n",
    "\n",
    "| 步骤 | 操作 | 形状变化 |\n",
    "|------|------|---------|\n",
    "| 1 | Q, K, V = W_Q/K/V(input) | (2,5,4) → (2,5,4) |\n",
    "| 2 | 拆多头 (view + transpose) | (2,5,4) → (2,2,5,2) |\n",
    "| 3 | scores = Q × K^T / √d_k | → (2,2,5,5) |\n",
    "| 4 | masked_fill | 屏蔽 padding 为 -1e9 |\n",
    "| 5 | softmax(dim=-1) | 归一化成权重 |\n",
    "| 6 | attn_weights × V | → (2,2,5,2) |\n",
    "| 7 | 拼接多头 (transpose + view) | → (2,5,4) |\n",
    "| 8 | W_O 输出投影 | → (2,5,4) |\n",
    "\n",
    "**关键点：**\n",
    "- **多头**：并行学习多种注意力模式\n",
    "- **缩放 √d_k**：防止点积值过大，softmax 梯度消失\n",
    "- **Mask**：屏蔽 padding，softmax 后权重为 0\n",
    "- **W_O**：融合多头信息，输出最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35n87bnvqzs",
   "metadata": {},
   "source": [
    "## 7.Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqfh8pmh6z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dim=4, hidden_dim=16\n",
      "FFN 输入 shape: torch.Size([2, 5, 4])\n",
      "FFN 输出 shape: torch.Size([2, 5, 4])\n",
      "FFN 输出:\n",
      " tensor([[[ 0.2508, -0.0602, -0.1821,  0.0766],\n",
      "         [ 0.2507, -0.0604, -0.1821,  0.0768],\n",
      "         [ 0.2211, -0.0753, -0.2127,  0.0526],\n",
      "         [ 0.2211, -0.0753, -0.2127,  0.0526],\n",
      "         [ 0.2211, -0.0753, -0.2127,  0.0526]],\n",
      "\n",
      "        [[ 0.2314, -0.0387, -0.1903,  0.0434],\n",
      "         [ 0.2299, -0.0459, -0.1951,  0.0413],\n",
      "         [ 0.2300, -0.0418, -0.1933,  0.0360],\n",
      "         [ 0.2333, -0.0514, -0.1960,  0.0472],\n",
      "         [ 0.2188, -0.0518, -0.2022,  0.0337]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Feed Forward Network (FFN)\n",
    "# 公式: FFN(x) = ReLU(xW1 + b1)W2 + b2\n",
    "# 每个位置独立经过两层全连接网络\n",
    "\n",
    "# 参数：隐藏层通常是 model_dim 的 4 倍\n",
    "hidden_dim = model_dim * 4  # 4 × 4 = 16\n",
    "\n",
    "print(f\"model_dim={model_dim}, hidden_dim={hidden_dim}\")\n",
    "\n",
    "# FFN 结构：扩大 → 激活 → 缩回\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(model_dim, hidden_dim),   # 4 → 16 (扩大) \n",
    "                #    ↑        ↑\n",
    "                # 输入维度  输出维度\n",
    "    nn.ReLU(),                          # 非线性激活\n",
    "    nn.Linear(hidden_dim, model_dim)    # 16 → 4 (缩回)\n",
    ")\n",
    "\n",
    "# 前向传播 (output 是 Multi-Head Attention 的输出)\n",
    "ffn_output = ffn(output)\n",
    "\n",
    "print(\"FFN 输入 shape:\", output.shape)       # (2, 5, 4)\n",
    "print(\"FFN 输出 shape:\", ffn_output.shape)   # (2, 5, 4)\n",
    "print(\"FFN 输出:\\n\", ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x2bpck6928",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Feed Forward Network 为每个位置独立引入非线性变换：\n",
    "\n",
    "```\n",
    "Attention = 收集信息（\"看\"其他位置）\n",
    "FFN = 处理信息（\"思考\"怎么用）\n",
    "```\n",
    "\n",
    "**公式：**\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**结构：**\n",
    "```\n",
    "输入 (4) → Linear (4→16) → ReLU → Linear (16→4) → 输出 (4)\n",
    "           扩大4倍        非线性     缩回\n",
    "```\n",
    "\n",
    "**代码：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1. 扩大 | `nn.Linear(model_dim, hidden_dim)` | 4 → 16 |\n",
    "| 2. 激活 | `nn.ReLU()` | 引入非线性 |\n",
    "| 3. 缩回 | `nn.Linear(hidden_dim, model_dim)` | 16 → 4 |\n",
    "\n",
    "**为什么需要 FFN：**\n",
    "- Attention 只是线性加权求和，需要非线性增强表达能力\n",
    "- 扩大维度增加参数量和模型容量\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "输入:  (2, 5, 4)\n",
    "         ↓\n",
    "       (2, 5, 16)  扩大\n",
    "         ↓\n",
    "输出:  (2, 5, 4)   缩回\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smn81w5p63",
   "metadata": {},
   "source": [
    "## 8.Layer Norm + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0sv22y56g2l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 残差连接:\n",
      "  src_input shape: torch.Size([2, 5, 4])\n",
      "  attention output shape: torch.Size([2, 5, 4])\n",
      "  残差后 shape: torch.Size([2, 5, 4])\n",
      "  LayerNorm后 shape: torch.Size([2, 5, 4])\n",
      "\n",
      "FFN 残差连接:\n",
      "  FFN 输入 shape: torch.Size([2, 5, 4])\n",
      "  FFN 输出 shape: torch.Size([2, 5, 4])\n",
      "  残差后 shape: torch.Size([2, 5, 4])\n",
      "  LayerNorm后 shape: torch.Size([2, 5, 4])\n",
      "\n",
      "最终 Encoder 输出:\n",
      " tensor([[[-1.0908, -0.2949,  1.6317, -0.2460],\n",
      "         [ 0.3699, -1.3899,  1.3499, -0.3299],\n",
      "         [ 0.9829, -1.6600,  0.1817,  0.4954],\n",
      "         [ 0.7696, -1.7103,  0.3287,  0.6120],\n",
      "         [ 0.4210, -1.7175,  0.5204,  0.7761]],\n",
      "\n",
      "        [[ 1.2405, -1.1272, -0.8261,  0.7128],\n",
      "         [ 0.4959, -0.3833, -1.3981,  1.2855],\n",
      "         [ 0.2647, -1.6458,  1.0562,  0.3249],\n",
      "         [ 0.5846, -1.2720, -0.6061,  1.2935],\n",
      "         [ 0.4247, -1.7206,  0.5489,  0.7470]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer Norm + Residual Connection\n",
    "# 残差连接：output = x + sublayer(x)\n",
    "# 层归一化：LayerNorm(x) = (x - μ) / σ * γ + β\n",
    "\n",
    "# Layer Normalization 层\n",
    "layer_norm1 = nn.LayerNorm(model_dim)  # Attention 后\n",
    "layer_norm2 = nn.LayerNorm(model_dim)  # FFN 后\n",
    "\n",
    "# ========== Attention + 残差 + LayerNorm ==========\n",
    "# output 是 Multi-Head Attention 的输出\n",
    "# src_input 是原始输入\n",
    "\n",
    "residual_output1 = src_input + output  # 残差连接\n",
    "norm_output1 = layer_norm1(residual_output1)  # LayerNorm\n",
    "\n",
    "print(\"Attention 残差连接:\")\n",
    "print(\"  src_input shape:\", src_input.shape)\n",
    "print(\"  attention output shape:\", output.shape)\n",
    "print(\"  残差后 shape:\", residual_output1.shape)\n",
    "print(\"  LayerNorm后 shape:\", norm_output1.shape)\n",
    "\n",
    "# ========== FFN + 残差 + LayerNorm ==========\n",
    "ffn_out = ffn(norm_output1)  # FFN\n",
    "residual_output2 = norm_output1 + ffn_out  # 残差连接\n",
    "norm_output2 = layer_norm2(residual_output2)  # LayerNorm\n",
    "\n",
    "print(\"\\nFFN 残差连接:\")\n",
    "print(\"  FFN 输入 shape:\", norm_output1.shape)\n",
    "print(\"  FFN 输出 shape:\", ffn_out.shape)\n",
    "print(\"  残差后 shape:\", residual_output2.shape)\n",
    "print(\"  LayerNorm后 shape:\", norm_output2.shape)\n",
    "\n",
    "print(\"\\n最终 Encoder 输出:\\n\", norm_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlbhd3a0omm",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Layer Norm + Residual 让深层网络训练更稳定：\n",
    "\n",
    "**残差连接（Residual Connection）：**\n",
    "```python\n",
    "output = x + sublayer(x)  # 输入直接加到输出\n",
    "```\n",
    "- 让梯度可以直接流回，防止梯度消失\n",
    "\n",
    "**层归一化（Layer Normalization）：**\n",
    "$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\times \\gamma + \\beta$$\n",
    "- 对每个位置的向量独立归一化\n",
    "- 让每层输入分布稳定，加速收敛\n",
    "\n",
    "**Encoder Block 完整流程：**\n",
    "```\n",
    "输入 x\n",
    "   ├────────────────┐\n",
    "   ↓                │\n",
    "Multi-Head Attention │\n",
    "   ↓                │\n",
    "   + ←──────────────┘  残差\n",
    "   ↓\n",
    "LayerNorm\n",
    "   ├────────────────┐\n",
    "   ↓                │\n",
    "  FFN               │\n",
    "   ↓                │\n",
    "   + ←──────────────┘  残差\n",
    "   ↓\n",
    "LayerNorm\n",
    "   ↓\n",
    "输出\n",
    "```\n",
    "\n",
    "**代码：**\n",
    "\n",
    "| 步骤 | 代码 | 说明 |\n",
    "|------|------|------|\n",
    "| 1 | `src_input + output` | Attention 残差 |\n",
    "| 2 | `layer_norm1(...)` | 归一化 |\n",
    "| 3 | `norm_output1 + ffn_out` | FFN 残差 |\n",
    "| 4 | `layer_norm2(...)` | 归一化 |\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "全程保持 (2, 5, 4) 不变\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tv96wp78",
   "metadata": {},
   "source": [
    "## 9.Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wjo8j3dztm9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 shape: torch.Size([2, 5, 4])\n",
      "Mask shape: torch.Size([2, 5, 5])\n",
      "Encoder Block 输出 shape: torch.Size([2, 5, 4])\n",
      "Encoder Block 输出:\n",
      " tensor([[[-1.3515,  1.0477,  0.8694, -0.5656],\n",
      "         [-0.2517,  0.3528,  1.3291, -1.4303],\n",
      "         [ 1.5393, -1.1465,  0.1491, -0.5419],\n",
      "         [ 1.0762, -1.5481,  0.6444, -0.1725],\n",
      "         [ 0.3411, -1.6756,  0.9718,  0.3626]],\n",
      "\n",
      "        [[ 0.9038,  0.7961, -1.5931, -0.1068],\n",
      "         [-0.0833,  0.9081, -1.6016,  0.7768],\n",
      "         [ 0.9641, -0.4443,  0.9102, -1.4300],\n",
      "         [ 0.0554, -0.3479, -1.2382,  1.5306],\n",
      "         [ 0.5222, -1.6758,  0.9329,  0.2206]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 先封装 Multi-Head Attention 类\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "        \n",
    "        # 四个线性层\n",
    "        self.W_Q = nn.Linear(model_dim, model_dim)\n",
    "        self.W_K = nn.Linear(model_dim, model_dim)\n",
    "        self.W_V = nn.Linear(model_dim, model_dim)\n",
    "        self.W_O = nn.Linear(model_dim, model_dim)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. 线性变换\n",
    "        Q = self.W_Q(query)\n",
    "        K = self.W_K(key)\n",
    "        V = self.W_V(value)\n",
    "        \n",
    "        # 2. 拆成多头 (batch, seq, model_dim) → (batch, heads, seq, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # 4. 应用 mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (batch, 1, seq, seq)\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        # 5. softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 6. 加权求和\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # 7. 拼接多头\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.model_dim)\n",
    "        \n",
    "        # 8. 输出投影\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 封装 Encoder Block 类\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, model_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention + 残差 + LayerNorm\n",
    "        attn_output = self.attention(x, x, x, mask)  # Self-Attention: Q=K=V=x\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # FFN + 残差 + LayerNorm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 测试 Encoder Block\n",
    "encoder_block = EncoderBlock(model_dim=4, num_heads=2, hidden_dim=16)\n",
    "\n",
    "# 输入\n",
    "print(\"输入 shape:\", src_input.shape)\n",
    "print(\"Mask shape:\", encoder_self_attn_mask.shape)\n",
    "\n",
    "# 前向传播\n",
    "encoder_output = encoder_block(src_input, encoder_self_attn_mask)\n",
    "\n",
    "print(\"Encoder Block 输出 shape:\", encoder_output.shape)\n",
    "print(\"Encoder Block 输出:\\n\", encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spx3i6oie1",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Encoder Block 把前面的组件封装成一个完整的模块：\n",
    "\n",
    "**结构：**\n",
    "```\n",
    "输入 x\n",
    "   ↓\n",
    "Multi-Head Self-Attention (Q=K=V=x)\n",
    "   ↓\n",
    "残差 + LayerNorm\n",
    "   ↓\n",
    "FFN\n",
    "   ↓\n",
    "残差 + LayerNorm\n",
    "   ↓\n",
    "输出\n",
    "```\n",
    "\n",
    "**两个类：**\n",
    "\n",
    "| 类 | 作用 |\n",
    "|------|------|\n",
    "| `MultiHeadAttention` | 封装多头注意力（第6部分） |\n",
    "| `EncoderBlock` | 封装完整 Encoder 层 |\n",
    "\n",
    "**EncoderBlock 参数：**\n",
    "\n",
    "| 参数 | 含义 |\n",
    "|------|------|\n",
    "| `model_dim` | 模型维度 (4) |\n",
    "| `num_heads` | 注意力头数 (2) |\n",
    "| `hidden_dim` | FFN 隐藏层维度 (16) |\n",
    "\n",
    "**使用方式：**\n",
    "```python\n",
    "encoder_block = EncoderBlock(model_dim=4, num_heads=2, hidden_dim=16)\n",
    "output = encoder_block(src_input, encoder_self_attn_mask)\n",
    "```\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "输入:  (2, 5, 4)\n",
    "输出:  (2, 5, 4)  # 形状不变\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rpfazdbkyx",
   "metadata": {},
   "source": [
    "## 10.Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4u8m7pyn4r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder 输入 shape: torch.Size([2, 5, 4])\n",
      "Encoder 输出 shape: torch.Size([2, 5, 4])\n",
      "Decoder Self-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "Cross-Attention Mask shape: torch.Size([2, 5, 5])\n",
      "\n",
      "Decoder Block 输出 shape: torch.Size([2, 5, 4])\n",
      "Decoder Block 输出:\n",
      " tensor([[[-1.6077,  0.6133, -0.0246,  1.0191],\n",
      "         [ 0.6884, -1.0575, -0.8970,  1.2662],\n",
      "         [ 1.2849,  0.3856, -1.4697, -0.2009],\n",
      "         [ 0.4652, -1.4051, -0.3569,  1.2968],\n",
      "         [ 0.7739, -1.3215, -0.5936,  1.1412]],\n",
      "\n",
      "        [[-1.6557,  0.7432,  0.0754,  0.8370],\n",
      "         [-1.6306,  0.4527,  0.1186,  1.0593],\n",
      "         [ 1.6533, -0.2737, -0.3433, -1.0363],\n",
      "         [ 1.3545, -1.0044, -0.9150,  0.5649],\n",
      "         [ 0.8124, -1.1181, -0.8579,  1.1636]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Decoder Block\n",
    "# 比 Encoder 多一个 Cross-Attention 层\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(model_dim, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(model_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, model_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.norm3 = nn.LayerNorm(model_dim)\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # 1. Masked Self-Attention + 残差 + LayerNorm\n",
    "        # Q=K=V=x，但要屏蔽未来位置\n",
    "        self_attn_output = self.self_attention(x, x, x, self_attn_mask)\n",
    "        x = self.norm1(x + self_attn_output)\n",
    "        \n",
    "        # 2. Cross-Attention + 残差 + LayerNorm\n",
    "        # Q 来自 Decoder，K/V 来自 Encoder\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, cross_attn_mask)\n",
    "        x = self.norm2(x + cross_attn_output)\n",
    "        \n",
    "        # 3. FFN + 残差 + LayerNorm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 测试 Decoder Block\n",
    "decoder_block = DecoderBlock(model_dim=4, num_heads=2, hidden_dim=16)\n",
    "\n",
    "# 输入\n",
    "# tgt_input: Decoder 的输入（目标序列 + 位置编码）\n",
    "# encoder_output: Encoder 的输出\n",
    "# decoder_self_attn_mask: Decoder 的 causal mask（第5部分）\n",
    "# cross_attn_mask: Cross-Attention 的 mask（第4部分）\n",
    "\n",
    "print(\"Decoder 输入 shape:\", tgt_input.shape)\n",
    "print(\"Encoder 输出 shape:\", encoder_output.shape)\n",
    "print(\"Decoder Self-Attention Mask shape:\", decoder_self_attn_mask.shape)\n",
    "print(\"Cross-Attention Mask shape:\", cross_attn_mask.shape)\n",
    "\n",
    "# 前向传播\n",
    "decoder_output = decoder_block(\n",
    "    tgt_input,              # Decoder 输入\n",
    "    encoder_output,         # Encoder 输出\n",
    "    decoder_self_attn_mask, # Causal Mask\n",
    "    cross_attn_mask         # Cross-Attention Mask\n",
    ")\n",
    "\n",
    "print(\"\\nDecoder Block 输出 shape:\", decoder_output.shape)\n",
    "print(\"Decoder Block 输出:\\n\", decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nt4ldmdeqx",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "Decoder Block 比 Encoder 多一个 Cross-Attention，用于\"看\"源序列：\n",
    "\n",
    "**结构对比：**\n",
    "\n",
    "| Encoder Block | Decoder Block |\n",
    "|---------------|---------------|\n",
    "| Self-Attention | Masked Self-Attention |\n",
    "| - | **Cross-Attention** |\n",
    "| FFN | FFN |\n",
    "\n",
    "**Decoder Block 流程：**\n",
    "```\n",
    "输入 x (目标序列)\n",
    "   ↓\n",
    "Masked Self-Attention (Q=K=V=x, 屏蔽未来)\n",
    "   ↓\n",
    "残差 + LayerNorm\n",
    "   ↓\n",
    "Cross-Attention (Q=x, K=V=encoder_output)\n",
    "   ↓\n",
    "残差 + LayerNorm\n",
    "   ↓\n",
    "FFN\n",
    "   ↓\n",
    "残差 + LayerNorm\n",
    "   ↓\n",
    "输出\n",
    "```\n",
    "\n",
    "**三个 Attention 的区别：**\n",
    "\n",
    "| 类型 | Q | K | V | Mask |\n",
    "|------|---|---|---|------|\n",
    "| Encoder Self-Attn | src | src | src | padding |\n",
    "| Decoder Self-Attn | tgt | tgt | tgt | padding + causal |\n",
    "| Cross-Attention | tgt | src | src | 双方 padding |\n",
    "\n",
    "**代码关键点：**\n",
    "\n",
    "```python\n",
    "# Masked Self-Attention: 不能看未来\n",
    "self.self_attention(x, x, x, self_attn_mask)\n",
    "\n",
    "# Cross-Attention: Q 来自 Decoder，K/V 来自 Encoder\n",
    "self.cross_attention(x, encoder_output, encoder_output, cross_attn_mask)\n",
    "```\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "tgt_input:       (2, 5, 4)   # Decoder 输入\n",
    "encoder_output:  (2, 5, 4)   # Encoder 输出\n",
    "decoder_output:  (2, 5, 4)   # Decoder 输出\n",
    "```\n",
    "\n",
    "**完整 Transformer = N × EncoderBlock + N × DecoderBlock + 输出层**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7h1wcj7go9",
   "metadata": {},
   "source": [
    "## 11.完整 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vhlxdg2z60m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 结构:\n",
      "  src_vocab_size: 9\n",
      "  tgt_vocab_size: 9\n",
      "  model_dim: 4\n",
      "  num_heads: 2\n",
      "  num_layers: 2\n",
      "\n",
      "输入:\n",
      "  src_seq shape: torch.Size([2, 5])\n",
      "  tgt_seq shape: torch.Size([2, 5])\n",
      "\n",
      "输出:\n",
      "  logits shape: torch.Size([2, 5, 9])\n",
      "\n",
      "预测概率 (softmax后):\n",
      "  样本0, 位置0 的词表概率分布: tensor([0.0492, 0.0364, 0.0952, 0.0246, 0.1352, 0.0628, 0.2921, 0.1024, 0.2021],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "  预测的词索引: 6\n"
     ]
    }
   ],
   "source": [
    "# 完整 Transformer\n",
    "# 结构: Encoder堆叠 + Decoder堆叠 + 输出层\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size,    # 源词表大小\n",
    "                 tgt_vocab_size,    # 目标词表大小\n",
    "                 model_dim=512,     # 模型维度\n",
    "                 num_heads=8,       # 注意力头数\n",
    "                 num_layers=6,      # Encoder/Decoder 层数\n",
    "                 hidden_dim=2048,   # FFN 隐藏层维度\n",
    "                 max_seq_len=5000,  # 最大序列长度\n",
    "                 dropout=0.1):      # Dropout 比例\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # ========== Embedding 层 ==========\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, model_dim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, model_dim)\n",
    "        \n",
    "        # 位置编码 (固定，不学习)\n",
    "        self.register_buffer('pos_encoding', self._sinusoidal_encoding(max_seq_len, model_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ========== Encoder 堆叠 ==========\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(model_dim, num_heads, hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # ========== Decoder 堆叠 ==========\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(model_dim, num_heads, hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # ========== 输出层 ==========\n",
    "        self.output_linear = nn.Linear(model_dim, tgt_vocab_size)\n",
    "    \n",
    "    def _sinusoidal_encoding(self, max_len, dim):\n",
    "        \"\"\"生成正弦位置编码\"\"\"\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        return pe\n",
    "    \n",
    "    def encode(self, src_seq, src_mask):\n",
    "        \"\"\"Encoder 前向传播\"\"\"\n",
    "        # Embedding + Position + Dropout\n",
    "        x = self.src_embedding(src_seq) * math.sqrt(self.model_dim)  # 缩放\n",
    "        x = x + self.pos_encoding[:src_seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过所有 Encoder 层\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt_seq, encoder_output, tgt_mask, cross_mask):\n",
    "        \"\"\"Decoder 前向传播\"\"\"\n",
    "        # Embedding + Position + Dropout\n",
    "        x = self.tgt_embedding(tgt_seq) * math.sqrt(self.model_dim)\n",
    "        x = x + self.pos_encoding[:tgt_seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过所有 Decoder 层\n",
    "        for decoder in self.decoder_layers:\n",
    "            x = decoder(x, encoder_output, tgt_mask, cross_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src_seq, tgt_seq, src_mask, tgt_mask, cross_mask):\n",
    "        \"\"\"完整前向传播\"\"\"\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(src_seq, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_output = self.decode(tgt_seq, encoder_output, tgt_mask, cross_mask)\n",
    "        \n",
    "        # 输出层: 映射到词表大小\n",
    "        logits = self.output_linear(decoder_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# ========== 测试完整 Transformer ==========\n",
    "# 使用之前定义的参数\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=max_num_src_words + 1,  # 9\n",
    "    tgt_vocab_size=max_num_tgt_words + 1,  # 9\n",
    "    model_dim=model_dim,                    # 4\n",
    "    num_heads=num_heads,                    # 2\n",
    "    num_layers=2,                           # 2层 (原论文6层)\n",
    "    hidden_dim=16,                          # 16\n",
    "    max_seq_len=100\n",
    ")\n",
    "\n",
    "print(\"Transformer 结构:\")\n",
    "print(f\"  src_vocab_size: {max_num_src_words + 1}\")\n",
    "print(f\"  tgt_vocab_size: {max_num_tgt_words + 1}\")\n",
    "print(f\"  model_dim: {model_dim}\")\n",
    "print(f\"  num_heads: {num_heads}\")\n",
    "print(f\"  num_layers: 2\")\n",
    "\n",
    "# 前向传播\n",
    "logits = transformer(\n",
    "    src_seq,                # (2, 5) 源序列\n",
    "    tgt_seq,                # (2, 5) 目标序列\n",
    "    encoder_self_attn_mask, # (2, 5, 5) Encoder mask\n",
    "    decoder_self_attn_mask, # (2, 5, 5) Decoder mask\n",
    "    cross_attn_mask         # (2, 5, 5) Cross-Attention mask\n",
    ")\n",
    "\n",
    "print(f\"\\n输入:\")\n",
    "print(f\"  src_seq shape: {src_seq.shape}\")\n",
    "print(f\"  tgt_seq shape: {tgt_seq.shape}\")\n",
    "print(f\"\\n输出:\")\n",
    "print(f\"  logits shape: {logits.shape}\")  # (batch, tgt_seq_len, tgt_vocab_size)\n",
    "print(f\"\\n预测概率 (softmax后):\")\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"  样本0, 位置0 的词表概率分布: {probs[0, 0]}\")\n",
    "print(f\"  预测的词索引: {torch.argmax(probs[0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ojxk9sg56",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "完整 Transformer 由 Embedding、Encoder 堆叠、Decoder 堆叠、输出层组成：\n",
    "\n",
    "**整体架构：**\n",
    "```\n",
    "源序列 src_seq                          目标序列 tgt_seq\n",
    "    ↓                                       ↓\n",
    "src_embedding                           tgt_embedding\n",
    "    ↓                                       ↓\n",
    " + pos_encoding                          + pos_encoding\n",
    "    ↓                                       ↓\n",
    "  Dropout                                 Dropout\n",
    "    ↓                                       ↓\n",
    "┌─────────────┐                      ┌─────────────┐\n",
    "│ EncoderBlock│ ×N                   │ DecoderBlock│ ×N\n",
    "│  - Self-Attn│ ──encoder_output──→  │  - Self-Attn│\n",
    "│  - FFN      │                      │  - Cross-Attn│\n",
    "└─────────────┘                      │  - FFN      │\n",
    "                                     └─────────────┘\n",
    "                                           ↓\n",
    "                                     output_linear\n",
    "                                           ↓\n",
    "                                     logits (词表概率)\n",
    "```\n",
    "\n",
    "**核心组件：**\n",
    "\n",
    "| 组件 | 作用 |\n",
    "|------|------|\n",
    "| `src/tgt_embedding` | 词索引 → 向量 |\n",
    "| `pos_encoding` | 注入位置信息 |\n",
    "| `encoder_layers` | N 个 EncoderBlock 堆叠 |\n",
    "| `decoder_layers` | N 个 DecoderBlock 堆叠 |\n",
    "| `output_linear` | 映射到词表大小，输出 logits |\n",
    "\n",
    "**三个方法：**\n",
    "\n",
    "| 方法 | 作用 |\n",
    "|------|------|\n",
    "| `encode()` | 编码源序列 |\n",
    "| `decode()` | 解码目标序列 |\n",
    "| `forward()` | 完整前向传播 |\n",
    "\n",
    "**Embedding 缩放：**\n",
    "```python\n",
    "x = self.src_embedding(src_seq) * math.sqrt(self.model_dim)\n",
    "```\n",
    "- 原论文做法，让 Embedding 和 Position Encoding 量级匹配\n",
    "\n",
    "**形状变化：**\n",
    "```\n",
    "src_seq:  (2, 5)           # 源序列词索引\n",
    "tgt_seq:  (2, 5)           # 目标序列词索引\n",
    "logits:   (2, 5, 9)        # 每个位置对词表的预测分数\n",
    "```\n",
    "\n",
    "**原论文参数 vs 我们的参数：**\n",
    "\n",
    "| 参数 | 原论文 | 本例 |\n",
    "|------|--------|------|\n",
    "| model_dim | 512 | 4 |\n",
    "| num_heads | 8 | 2 |\n",
    "| num_layers | 6 | 2 |\n",
    "| hidden_dim | 2048 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4hvdtrsm",
   "metadata": {},
   "source": [
    "## 12.RoPE 旋转位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hywpt7jdf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原向量: tensor([1., 0.])\n",
      "旋转90度后: tensor([6.1232e-17, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# RoPE: Rotary Position Embedding（旋转位置编码）\n",
    "# 核心思想：用旋转矩阵编码位置，让注意力分数自然包含相对位置信息\n",
    "\n",
    "# ========== 回顾：正弦位置编码的问题 ==========\n",
    "# 正弦编码：Position Embedding + Word Embedding（加法）\n",
    "# 问题：位置信息和语义信息混在一起，不够优雅\n",
    "\n",
    "# ========== RoPE 的核心思想 ==========\n",
    "# 1. 不是\"加\"位置信息，而是\"旋转\"向量\n",
    "# 2. 位置 m 的向量旋转 m*θ 角度\n",
    "# 3. Q 和 K 旋转后，点积自然包含相对位置 (m-n)\n",
    "\n",
    "# ========== 2D 旋转的直觉 ==========\n",
    "# 把 2D 向量 [x, y] 旋转 θ 角度：\n",
    "# [x', y'] = [x*cos(θ) - y*sin(θ), x*sin(θ) + y*cos(θ)]\n",
    "#\n",
    "# 用矩阵表示：\n",
    "# [x']   [cos(θ)  -sin(θ)] [x]\n",
    "# [y'] = [sin(θ)   cos(θ)] [y]\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 先看 2D 旋转的例子\n",
    "def rotate_2d(x, theta):\n",
    "    \"\"\"将 2D 向量旋转 theta 角度\"\"\"\n",
    "    cos = math.cos(theta)\n",
    "    sin = math.sin(theta)\n",
    "    x_new = x[0] * cos - x[1] * sin\n",
    "    y_new = x[0] * sin + x[1] * cos\n",
    "    return torch.tensor([x_new, y_new])\n",
    "\n",
    "# 示例：向量 [1, 0] 旋转 90 度\n",
    "v = torch.tensor([1.0, 0.0])\n",
    "v_rotated = rotate_2d(v, math.pi / 2)  # 90度 = π/2\n",
    "print(f\"原向量: {v}\")\n",
    "print(f\"旋转90度后: {v_rotated}\")  # 应该是 [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rrd21zlt1dp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始点积: 0.9500\n",
      "旋转后点积: 0.9371\n",
      "相对位置 m-n = -3\n"
     ]
    }
   ],
   "source": [
    "# ========== RoPE 的关键性质 ==========\n",
    "# 位置 m 的 Q 旋转 m*θ，位置 n 的 K 旋转 n*θ\n",
    "# Q_m · K_n = |Q||K| * cos((m-n)*θ)\n",
    "# \n",
    "# 点积结果只和\"相对位置 (m-n)\"有关！\n",
    "\n",
    "# 验证：两个向量旋转后的点积\n",
    "def dot_product(a, b):\n",
    "    return (a * b).sum()\n",
    "\n",
    "# 原始向量\n",
    "q = torch.tensor([1.0, 0.5])\n",
    "k = torch.tensor([0.8, 0.3])\n",
    "\n",
    "# 位置 m=2, n=5，基础角度 θ=0.1\n",
    "theta = 0.1\n",
    "m, n = 2, 5\n",
    "\n",
    "# 分别旋转\n",
    "q_rotated = rotate_2d(q, m * theta)  # Q 旋转 m*θ\n",
    "k_rotated = rotate_2d(k, n * theta)  # K 旋转 n*θ\n",
    "\n",
    "# 计算点积\n",
    "dot_original = dot_product(q, k)\n",
    "dot_rotated = dot_product(q_rotated, k_rotated)\n",
    "\n",
    "print(f\"原始点积: {dot_original:.4f}\")\n",
    "print(f\"旋转后点积: {dot_rotated:.4f}\")\n",
    "print(f\"相对位置 m-n = {m-n}\")\n",
    "\n",
    "# 关键观察：旋转后的点积 = 原始点积 * cos((m-n)*θ) + 交叉项\n",
    "# 这意味着注意力分数自然编码了相对位置！"
   ]
  },
  {
   "cell_type": "code",
   "id": "gvw8lohn2pr",
   "source": "# ========== 高维 RoPE 实现 ==========\n# 把 D 维向量分成 D/2 组，每组 2 维，分别旋转\n# 每组用不同的频率 θ_i = 10000^(-2i/D)\n\ndef precompute_rope_freqs(dim, max_seq_len, base=10000.0):\n    \"\"\"\n    预计算 RoPE 的频率和角度\n    dim: 向量维度（必须是偶数）\n    max_seq_len: 最大序列长度\n    base: 基础频率（默认 10000）\n    \"\"\"\n    # 频率：θ_i = 1 / (base^(2i/dim))\n    # i = 0, 1, 2, ..., dim/2 - 1\n    freqs = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n    \n    # 位置：0, 1, 2, ..., max_seq_len - 1\n    positions = torch.arange(max_seq_len).float()\n    \n    # 角度矩阵：(seq_len, dim/2)\n    # angles[pos, i] = pos * θ_i\n    angles = torch.outer(positions, freqs)\n    \n    # 返回 cos 和 sin\n    return torch.cos(angles), torch.sin(angles)\n\n# 测试\ndim = 4\nmax_len = 5\ncos_cached, sin_cached = precompute_rope_freqs(dim, max_len)\n\nprint(f\"维度: {dim}, 最大长度: {max_len}\")\nprint(f\"cos shape: {cos_cached.shape}\")  # (5, 2)\nprint(f\"cos:\\n{cos_cached}\")\nprint(f\"\\nsin:\\n{sin_cached}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yyzdgnjen",
   "source": "# ========== 应用 RoPE 到向量 ==========\n# 把向量分成两半：[x0, x1, x2, x3] → [x0, x1] 和 [x2, x3]\n# 每对做 2D 旋转\n\ndef apply_rope(x, cos, sin):\n    \"\"\"\n    对输入 x 应用 RoPE\n    x: (batch, seq_len, dim) 或 (batch, heads, seq_len, head_dim)\n    cos, sin: (seq_len, dim/2)\n    \"\"\"\n    # 获取序列长度\n    seq_len = x.shape[-2]\n    dim = x.shape[-1]\n    \n    # 取对应长度的 cos/sin\n    cos = cos[:seq_len]  # (seq_len, dim/2)\n    sin = sin[:seq_len]\n    \n    # 把 x 分成两半\n    x1 = x[..., :dim//2]  # 前半部分\n    x2 = x[..., dim//2:]  # 后半部分\n    \n    # 旋转公式：\n    # x1_new = x1 * cos - x2 * sin\n    # x2_new = x1 * sin + x2 * cos\n    x1_new = x1 * cos - x2 * sin\n    x2_new = x1 * sin + x2 * cos\n    \n    # 拼接回去\n    return torch.cat([x1_new, x2_new], dim=-1)\n\n# 测试\nbatch_size = 2\nseq_len = 5\ndim = 4\n\n# 随机输入\nx = torch.randn(batch_size, seq_len, dim)\nprint(f\"输入 x shape: {x.shape}\")\nprint(f\"输入 x[0]:\\n{x[0]}\")\n\n# 应用 RoPE\nx_rope = apply_rope(x, cos_cached, sin_cached)\nprint(f\"\\nRoPE 后 shape: {x_rope.shape}\")\nprint(f\"RoPE 后 x[0]:\\n{x_rope[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "k7mlqx4gs5s",
   "source": "# ========== 带 RoPE 的 Multi-Head Attention ==========\n\nclass RoPEMultiHeadAttention(nn.Module):\n    def __init__(self, model_dim, num_heads, max_seq_len=512):\n        super().__init__()\n        self.model_dim = model_dim\n        self.num_heads = num_heads\n        self.head_dim = model_dim // num_heads\n        \n        self.W_Q = nn.Linear(model_dim, model_dim)\n        self.W_K = nn.Linear(model_dim, model_dim)\n        self.W_V = nn.Linear(model_dim, model_dim)\n        self.W_O = nn.Linear(model_dim, model_dim)\n        \n        # 预计算 RoPE 的 cos/sin\n        cos, sin = precompute_rope_freqs(self.head_dim, max_seq_len)\n        self.register_buffer('cos_cached', cos)\n        self.register_buffer('sin_cached', sin)\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # 1. 线性变换\n        Q = self.W_Q(query)\n        K = self.W_K(key)\n        V = self.W_V(value)\n        \n        # 2. 拆成多头 (batch, seq, dim) → (batch, heads, seq, head_dim)\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # 3. 对 Q 和 K 应用 RoPE（V 不需要）\n        Q = apply_rope(Q, self.cos_cached, self.sin_cached)\n        K = apply_rope(K, self.cos_cached, self.sin_cached)\n        \n        # 4. 计算注意力分数\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # 5. 应用 mask\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            scores = scores.masked_fill(~mask, -1e9)\n        \n        # 6. softmax\n        attn_weights = F.softmax(scores, dim=-1)\n        \n        # 7. 加权求和\n        attn_output = torch.matmul(attn_weights, V)\n        \n        # 8. 拼接多头\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.model_dim)\n        \n        # 9. 输出投影\n        return self.W_O(attn_output)\n\n\n# 测试\nrope_attn = RoPEMultiHeadAttention(model_dim=4, num_heads=2, max_seq_len=100)\n\n# 使用之前的输入\noutput = rope_attn(src_input, src_input, src_input, encoder_self_attn_mask)\n\nprint(f\"输入 shape: {src_input.shape}\")\nprint(f\"输出 shape: {output.shape}\")\nprint(f\"输出:\\n{output}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i7zyn6u3hmr",
   "source": "### 总结\n\nRoPE（Rotary Position Embedding）用旋转代替加法编码位置：\n\n**正弦编码 vs RoPE：**\n\n| 对比 | 正弦编码 | RoPE |\n|------|----------|------|\n| 方式 | 加法：`x + PE` | 旋转：`rotate(x, θ)` |\n| 位置类型 | 绝对位置 | **相对位置** |\n| 应用对象 | Embedding | **Q 和 K** |\n| 使用模型 | 原版 Transformer | LLaMA, Qwen, ChatGLM |\n\n**核心思想：**\n```\n位置 m 的 Q 旋转 m*θ\n位置 n 的 K 旋转 n*θ\n点积 Q·K 自然包含相对位置 (m-n)\n```\n\n**2D 旋转公式：**\n```\n[x']   [cos(θ)  -sin(θ)] [x]\n[y'] = [sin(θ)   cos(θ)] [y]\n```\n\n**高维实现：**\n- 把 D 维分成 D/2 组，每组 2 维\n- 每组用不同频率 θ_i = 1/10000^(2i/D)\n- 分别做 2D 旋转\n\n**代码关键：**\n```python\n# 1. 预计算频率\nfreqs = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))\nangles = positions * freqs  # (seq_len, dim/2)\n\n# 2. 应用旋转（只对 Q 和 K）\nx1_new = x1 * cos - x2 * sin\nx2_new = x1 * sin + x2 * cos\n```\n\n**为什么 V 不需要 RoPE？**\n- RoPE 的目的是让 Q·K 包含相对位置\n- V 是被加权求和的内容，不参与位置计算\n\n**RoPE 的优势：**\n1. 相对位置：注意力分数只和距离有关\n2. 外推性好：能处理比训练更长的序列\n3. 计算高效：只需要逐元素乘法",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}